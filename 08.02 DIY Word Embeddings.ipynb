{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scipy scikit-learn plotly tqdm\n",
    "%pip install --no-cache-dir --force-reinstall https://dm.cs.tu-dortmund.de/nats/nats25_08_01_diy_word_embeddings-0.1-py3-none-any.whl\n",
    "import nats25_08_01_diy_word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a0381",
   "metadata": {},
   "source": [
    "# DIY Word Embeddings with a simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1281e43",
   "metadata": {},
   "source": [
    "## Hello Backpropagation\n",
    "\n",
    "In this assignment, you will be asked to train your own neural network. Therefore, you will be asked to write your own backpropagation implementation.\n",
    "In this notebook, we will try to keep things close to `PyTorch`. To help you write a similar API, you are given the following abstract classes for loss functions, network modules (activation functions and utility) and trainable modules (layers).\n",
    "In derived classes (designated by the class name in brackets), you will then in most cases only need to override the functions annotated with `@abstractmethod`.\n",
    "\n",
    "Read the code closely and get familiar with these concepts:\n",
    "- `forward` is the feed-forward functionality of the network. Layers and activation functions should modify the inputs and return them. All inputs are cached in a local variable for use in gradient descent steps.\n",
    "- `backward` computes gradients derived from a given `delta` (target direction of change in output) and returns the `next_delta` (target direction of change in input). For loss functions, it should return the target direction of change in input for a given prediction and target value. The computed gradients (if necessary) are again stored in a local variable.\n",
    "- `step_gradient` is used to add the last gradient computed with `backward` multiplied by the learning rate to the parameters of this module. For a single gradient descent step, you will need to executed `forward` -> `backward` -> `step_gradient`.\n",
    "- Functions preceded with an underscore are the module-specific functionality that remains to be implemented in derived classes.\n",
    "\n",
    "Cross reference example code of `PyTorch` to see similarities and differences of these approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, prediction, target): pass\n",
    "    @abstractmethod\n",
    "    def backward(self, prediction, target): pass\n",
    "class NetworkModule(ABC):\n",
    "    @abstractmethod\n",
    "    def _forward(self, X):\n",
    "        return None\n",
    "    def forward(self, X):\n",
    "        self.last_input = X.copy()\n",
    "        return self._forward(X)\n",
    "    @abstractmethod\n",
    "    def backward(self, delta): pass\n",
    "    def step_gradient(self, learning_rate): pass\n",
    "class TrainableModule(NetworkModule):\n",
    "    def __init__(self):\n",
    "        self.last_input = None\n",
    "        self.grad = None\n",
    "    def backward(self, delta):\n",
    "        if self.last_input is None:\n",
    "            raise AssertionError(\"Tried to execute backpropagation without forward feeding data.\")\n",
    "        next_delta = self._next_delta(delta)\n",
    "        self.grad = self._gradient_for_last_input(delta)\n",
    "        return next_delta\n",
    "    @abstractmethod\n",
    "    def _next_delta(self, delta):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def _gradient_for_last_input(self, delta):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def step_gradient(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a00764",
   "metadata": {},
   "source": [
    "When training networks with multiple layers, you will want to aggregate the functionality (e.g. `forward`) into a single call.\n",
    "The simplest way to do this, is to just have a chain of modules that are processed one after the other.\n",
    "Implement the missing functions in the `ModuleChain` class below.\n",
    "\n",
    "**Take care:** `numpy` arrays are stored as references! Inplace math operations like `+=` or `*=` do not create a copy but override the original matrix. You *do not* want to override your inputs but *may want* to override your attributes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleChain(NetworkModule):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "    def _forward(self, X):\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        pass # Your solution here\n",
    "    def step_gradient(self, learning_rate):\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8a3df",
   "metadata": {},
   "source": [
    "Next, you will at least need fully connected linear layers consisting of a weight matrix $W$ and a bias $b$ that implement the function\n",
    "$$ f(x) = x^TW + b $$\n",
    "All weights and biases should be initialized at random. Make sure, that each column sums to 1.\n",
    "This way, the norm of latent vectors will not explode and training is a bit easier.\n",
    "\n",
    "Aside from the initialization and forward step, you will need to implement your first parts of backpropagation here.\n",
    "For the backpropagation, you need to decompose the gradient for the entire network into gradients per module.\n",
    "To do so, we can start with the derivation\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial x_0}\n",
    "= \\frac{\\partial x_1}{\\partial x_0} \\cdot \\frac{\\partial E}{\\partial x_1}\n",
    "= \\frac{\\partial x_1}{\\partial x_0} \\cdot \\ldots \\cdot \\frac{\\partial E}{\\partial x_n}\n",
    "$$\n",
    "where $x_0$ is the input vector and the other $x_i$ denote the latent vectors after forwarding through $i$ modules.\n",
    "This allows to simplify for the $k$-th layer as\n",
    "$$\n",
    "\\delta_k\n",
    "= \\frac{\\partial E}{\\partial x_k}\n",
    "= \\frac{\\partial x_{k+1}}{\\partial x_k} \\cdot \\frac{\\partial E}{\\partial x_{k+1}}\n",
    "= \\frac{\\partial x_{k+1}}{\\partial x_k} \\cdot \\delta_{k+1}\n",
    "$$\n",
    "That is, the `next_delta` (in input direction) is the product of the derivation of the module function evaluated at the `last_input` multiplied by the `delta` of the layer afterwards, where we consider \"next\" to move from last to first layer. (The \"multiplied by\" does a lot of heavy lifting here, when `delta` is vector-valued. This is *not* a scalar or Hadamard product!)\n",
    "\n",
    "Similarly, we can decompose the gradient for the weights and the bias of this module\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W}\n",
    "= \\frac{\\partial x_{k+1}}{\\partial W} \\cdot \\delta_{k+1}\n",
    "\\quad\\quad\\quad\n",
    "\\frac{\\partial E}{\\partial b}\n",
    "= \\frac{\\partial x_{k+1}}{\\partial b} \\cdot \\delta_{k+1}\n",
    "$$\n",
    "These equations are *hand-waving* a lot of the details! Strictly speaking, we can not pull the products apart for entire vectors (at least its not a scalar product anymore). It is an easy way of remembering the right steps, though, and you only need to add a bunch of indices to each of the equations (or change the product types) to make them right. The indices just make it unreadable. Try to figure out where to add which index and add code to the functions below as you see fit. These equations here, however, can be translated to vector and matrix operations, so that you do not need to compute the gradient for each coefficient individually.\n",
    "\n",
    "*Hint 1:* The $\\delta_i$ are vectors of the same shape as the output of each module (i.e. shape of the input of the next module in forward direction). All gradients must have the same shape as the parameters. That allows for only very specific vector and matrix operations.\n",
    "\n",
    "*Hint 2:* Libraries like `PyTorch` allow to either pass a single input vector or a vector of input vectors through the models. Ensuring that functionality can be a bit tricky, so it is okay to always require single input vectors for simplicity. Just keep that constraint in mind when training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(TrainableModule):\n",
    "    def __init__(self, n_in, n_out, use_bias=True):\n",
    "        '''\n",
    "        Creates a fully connected linear layer translating from vectors of length `n_in` to vectors of length `n_out`.\n",
    "        `use_bias` controls whether or not this layer should use a bias (`f(x) = x^T W+b`) or not (`f(x) = x^T W`).\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # self.weights = ...\n",
    "        # self.bias = ...\n",
    "        # self.use_bias = ...\n",
    "        pass # Your solution here\n",
    "    def _forward(self, X):\n",
    "        pass # Your solution here\n",
    "    def _next_delta(self, delta):\n",
    "        pass # Your solution here\n",
    "    def _gradient_for_last_input(self, delta):\n",
    "        pass # Your solution here\n",
    "    def step_gradient(self, learning_rate):\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d4578",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_diy_word_embeddings.hidden_tests_8_0(Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a946dd",
   "metadata": {},
   "source": [
    "The next step is to define some activation functions and a loss. You will later need the Softmax activation and the Cross Entropy loss, but these are a bit tricky to implement. Instead, start with [ReLU](https://en.wikipedia.org/wiki/Rectified_linear_unit), [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) and L2 loss (half of the squared Euclidean distance between desired output and observed output).\n",
    "\n",
    "*Hint:* Here again, you need to return the `delta` for the next layer in input direction, which follows the same rule (derivative evaluated at last input times `delta` of next layer in output direction) for activation functions. For loss functions, you receive predicted and ground truth values and must return just the derivative in the input. Beware of the sign of the L2 loss derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(NetworkModule):\n",
    "    def _forward(self, X):\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        pass # Your solution here\n",
    "class Sigmoid(NetworkModule):\n",
    "    def _forward(self, X):\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        pass # Your solution here\n",
    "class L2Loss(Loss):\n",
    "    def forward(self, prediction, target):\n",
    "        pass # Your solution here\n",
    "    def backward(self, prediction, target):\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa9ba3",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_diy_word_embeddings.hidden_tests_11_0(ReLU, L2Loss, Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d6ce9",
   "metadata": {},
   "source": [
    "Now that you have the fundamental building blocks of a neural network, you can debug your code using a simple example, that some simple architecture should be able to solve and that can be visualized easily. Complete the code below to train the prescribed architecture. Depending on your implementation, each gradient step can be performed on a single input-output pair or on multiples (which is quite a bit trickier). In each iteration choose inputs and outputs for a gradient step, compute the gradient via `forward` and `backward` and call the `step_gradient` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset.\n",
    "X,y = make_moons(1000, noise=.2)\n",
    "# Initializing a module chain for a simple neural network with multiple hidden layers.\n",
    "# To debug only some of the module classes, create different architectures,\n",
    "# but beware that you might need to change the learning rate then.\n",
    "model = ModuleChain([Linear(2, 50), ReLU(), Linear(50, 20), ReLU(), Linear(20, 1), Sigmoid()])\n",
    "loss = L2Loss()\n",
    "learning_rate = 1e-1\n",
    "n_iterations = 50000\n",
    "with tqdm(range(n_iterations), total=n_iterations) as bar:\n",
    "    for _ in bar:\n",
    "        # Insert code for a single training iteration.\n",
    "        # You can track some information, by overriding `bar.desc`\n",
    "        # which is the description string displayed on the tqdm\n",
    "        # progress bar.\n",
    "        pass # Your solution here\n",
    "# Compute the predicted label for all outputs and the accuracy of the model.\n",
    "# y_pred = ...\n",
    "# accuracy = ...\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of your label prediction.\n",
    "# Change the number of digits for rounding to go from a\n",
    "# continuous visualization to class prediction.\n",
    "marker_label = np.round(y_pred, 3)\n",
    "go.Figure(\n",
    "    go.Scatter(\n",
    "        x=X[:,0],\n",
    "        y=X[:,1],\n",
    "        mode=\"markers\",\n",
    "        text=marker_label,\n",
    "        marker_color=marker_label,\n",
    "    ),\n",
    "    layout_yaxis_scaleanchor=\"x\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb1b98",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_diy_word_embeddings.hidden_tests_15_0(accuracy, y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae536574",
   "metadata": {},
   "source": [
    "## A simple word embedding model\n",
    "\n",
    "We can now add the missing functions to train a very simple HMM-style word embedding model.\n",
    "The model is supposed to predict the next word after receiving the $k$ previous words.\n",
    "The entire model should compute the function\n",
    "\n",
    "$$\n",
    "Softmax((ReLU((x^TE)^TW_1+b_1)^TW_2+b_2)^TE^T)\n",
    "$$\n",
    "\n",
    "that is, we have the architecture `Linear (1)` -> `Linear (2)` -> `ReLU` -> `Linear(3)` -> `Linear (1) Transposed` -> `Softmax`, where `Linear (1)` does not use a bias.\n",
    "After training, we expect the weights $E$ of `Linear (1)` to contain our word embedding vectors.\n",
    "Linking the weights of the first and last layer theoretically makes the function bilinear and the derivatives a lot harder to compute.\n",
    "We, therefore, act like these are independent matrices during `backward` and add both gradients during `step_gradient`.\n",
    "This is technically not correct, but gradient descent with a sufficiently small learning rate typically \"survives\" such stunts.\n",
    "This is why we designed `backward` and `step_gradient` to be different steps, though, since otherwise the first layer would use a corrupted weight matrix for gradient computation.\n",
    "\n",
    "Complete the classes below to create new modules that \"borrow\" the weights (but not bias) of another linear module.\n",
    "To borrow, you can replace the weight matrix created in the `Linear` constructor with a (transposed) view by referencing `other_linear.weights` or `other_linear.weights.T`.\n",
    "The transpose of a matrix is a view, that references the original matrix and applies all inplace operations to that part of memory.\n",
    "\n",
    "*But beware:* To keep these layers synchronized, all changes to the weights must be made in place, i.e. using operators like `+=`. You may need to adapt your `Linear` implementation to satisfy that constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedLinear(Linear):\n",
    "    def __init__(self, other_linear, use_bias=True):\n",
    "        # You probably want to start by invoking the `Linear` constructor.\n",
    "        # super().__init__(...)\n",
    "        pass # Your solution here\n",
    "class LinkedTransposedLinear(Linear):\n",
    "    def __init__(self, other_linear, use_bias=True):\n",
    "        # You probably want to start by invoking the `Linear` constructor.\n",
    "        # super().__init__(...)\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca1733",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_diy_word_embeddings.hidden_tests_18_0(LinkedTransposedLinear, Linear, LinkedLinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80276409",
   "metadata": {},
   "source": [
    "Next we will need the [Softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function and the Cross Entropy loss.\n",
    "Implementing these can be a bit tricky, especially when computing the derivative.\n",
    "The Softmax function and its derivative are\n",
    "$$\n",
    "Softmax(x) = \\left(\\frac{e^{x_1}}{\\sum_{j} e^{x_j}}, \\ldots, \\frac{e^{x_d}}{\\sum_{j} e^{x_j}}\\right)\n",
    "\\\\\n",
    "\\nabla Softmax(x) = \\left(\n",
    "    \\left\\{\\begin{array}{rcl}\n",
    "        Softmax(x)_i - Softmax(x)_i \\cdot Softmax(x)_j &\\text{if}& i = j\\\\\n",
    "        - Softmax(x)_i \\cdot Softmax(x)_j &\\text{if}& i \\neq j\\\\\n",
    "    \\end{array}\\right.\n",
    "\\right)_{i,j}\n",
    "= diag(Softmax(x)) - Softmax(x) \\otimes Softmax(x)\n",
    "$$\n",
    "where $diag(v)$ is the diagonal matrix with $v$ on the diagonal and $\\otimes$ is the outer product.\n",
    "\n",
    "The Cross Entropy loss of a prediction $y$ and the ground_truth $y^{\\ast}$ and its derivative are\n",
    "$$\n",
    "CE(y, y^{\\ast}) = -\\sum_{i} y^{\\ast}_i \\log(y_i) = -(y^{\\ast})^T\\log^\\circ(y)\n",
    "\\\\\n",
    "\\nabla CE(y, y^{\\ast}) = \\left(-\\frac{y^{\\ast}_1}{y_1},\\ldots,-\\frac{y^{\\ast}_d}{y_d}\\right)\n",
    "$$\n",
    "where $\\log^\\circ$ is the componentwise logarithm.\n",
    "\n",
    "We could just implement both of these functions and start training **but** when implementing $\\nabla Softmax(x)$ naively, we would compute a $d_{out} \\times d_{out}$ matrix! Since the last layer produces a value for each word in our dictionary, we would have a space complexity of the dictionary size squared, which is way too much (probably billions and beyond)!\n",
    "But $y^{\\ast}$ will be a one-hot encoding. The resulting $\\delta$ will, therefore, only have one non-zero value.\n",
    "We also only need the derivative of Softmax multiplied with $\\delta$, i.e. $\\nabla Softmax(x) \\cdot \\delta$.\n",
    "Since $\\delta$ can only have one non-zero value, we can replace that matrix-vector-product with a vector-scalar product, using only the relevant row of $\\nabla Softmax(x)$.\n",
    "\n",
    "To implement that simplified behavior, we can make use of sparse matrices like [`scipy.sparse.csr_array`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_array.html) which implements the [Compressed sparse row](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)) format.\n",
    "By iterating over `zip(x.indptr, x.indices, x.data)`, you can receive the row, column and value of all non-zero values stored in a `csr_array` `x`.\n",
    "You can also perform `numpy`-style operations like `x.dot(...)` using that sparse array.\n",
    "`csr_array`s are always two dimensional, so you have to store vectors as $d \\times 1$ or $1 \\times d$ matrices.\n",
    "You can create a `csr_array` using the constructor `csr_array((values, (row_indices, col_indices)), shape=(n, m))` where `values`, `row_indices` and `col_indices` must be vectors of equal size denoting in which row and column each value lies.\n",
    "\n",
    "Now use all the information in this text block to implement the Softmax activation and the Cross Entropy loss classes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2082a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_array\n",
    "class Softmax(NetworkModule):\n",
    "    def _forward(self, X):\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        assert type(delta) == csr_array\n",
    "        pass # Your solution here\n",
    "class CELoss(Loss):\n",
    "    def forward(self, prediction, target):\n",
    "        pass # Your solution here\n",
    "    def backward(self, prediction, target):\n",
    "        assert type(target) == csr_array\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16dd4e",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_diy_word_embeddings.hidden_tests_21_0(Softmax, CELoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1167171",
   "metadata": {},
   "source": [
    "You will also need a way to translate words to one-hot encoded sparse vectors and back.\n",
    "To do so, complete the class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa198ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotDict():\n",
    "    def __init__(self, vocabulary):\n",
    "        # Ensure appropriate types for ordered access\n",
    "        # self.vocabulary = ...\n",
    "        pass # Your solution here\n",
    "    def word_to_one_hot(self, word):\n",
    "        pass # Your solution here\n",
    "    def one_hot_to_word(self, one_hot):\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1704ad8",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_diy_word_embeddings.hidden_tests_24_0(OneHotDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020bbb8",
   "metadata": {},
   "source": [
    "Again, verify your implementation by training on the simple example below.\n",
    "You will need to adapt your previous code to translate from the one-hot encodings to class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0592fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset.\n",
    "X,y = make_moons(1000, noise=.2)\n",
    "ohd = OneHotDict(np.unique(y))\n",
    "# Initializing a module chain for a simple neural network with multiple hidden layers.\n",
    "# To debug only some of the module classes, create different architectures,\n",
    "# but beware that you might need to change the learning rate then.\n",
    "model = ModuleChain([Linear(2, 50), ReLU(), Linear(50, 20), ReLU(), Linear(20, 2), Softmax()])\n",
    "loss = CELoss()\n",
    "learning_rate = 1e-1\n",
    "n_iterations = 50000\n",
    "with tqdm(range(n_iterations), total=n_iterations) as bar:\n",
    "    for _ in bar:\n",
    "        # Insert code for a single training iteration.\n",
    "        # You can track some information, by overriding `bar.desc`\n",
    "        # which is the description string displayed on the tqdm\n",
    "        # progress bar.\n",
    "        pass # Your solution here\n",
    "# Compute the predicted label for all outputs and the accuracy of the model.\n",
    "# y_pred = ...\n",
    "# accuracy = ...\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242475df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of your label prediction.\n",
    "# This time, rounding is not necessary, since predictions are categorical.\n",
    "marker_label = y_pred\n",
    "go.Figure(\n",
    "    go.Scatter(\n",
    "        x=X[:,0],\n",
    "        y=X[:,1],\n",
    "        mode=\"markers\",\n",
    "        text=marker_label,\n",
    "        marker_color=marker_label,\n",
    "    ),\n",
    "    layout_yaxis_scaleanchor=\"x\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c6a69",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_diy_word_embeddings.hidden_tests_28_0(accuracy, y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6672f",
   "metadata": {},
   "source": [
    "We finally arrive at the HMM-style part of our model. We will use a very simple approach, where the next word is predicted from the embedding vectors of the previous $k$ words - no recurrency, no transformers.\n",
    "For your first model we will keep it as simple as the few hundred lines above.\n",
    "But to make the next word dependent on the previous $k$ words, we will need to combine the latent representations of these $k$ words.\n",
    "One simple way to do so, is to concatenate the words, for which we need one last module.\n",
    "Add code to the class below to create a module that distributes a set of inputs to component modules and outputs a concatenation of their outputs.\n",
    "During the `backward` step, it needs to split the given `delta` into parts and feed them through the respective child modules.\n",
    "You can assume, that this module is always used at the start of a model, thus does not need to return a `delta` and that the outputs of all nested modules are of equal size (relevant for splitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleConcat(NetworkModule):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "    def _forward(self, X):\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        pass # Your solution here\n",
    "    def step_gradient(self, learning_rate):\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e4a37",
   "metadata": {},
   "source": [
    "Before training the model, we now need to load the dataset and prepare our `OneHotDict`.\n",
    "Complete the code below to load the plain text of Hamlet, tokenize each sentence, pad the tokenized sentences with $k$ times a padding token in front and back and initialize the `OneHotDict`.\n",
    "The padding will later be used to start sentences and to recognize the end of a generated sentence.\n",
    "Also remove all \"sentences\" with two or less tokens, since these are mostly directing instructions and we do not want to generate those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbbootstrap\n",
    "file_path = await nbbootstrap.ensure_resource(\"https://dm.cs.tu-dortmund.de/nats/data/hamlet.txt\")\n",
    "with open(file_path, \"rt\") as file:\n",
    "    full = file.read()\n",
    "import nbbootstrap, re\n",
    "sentence_regex = re.compile(r\"[.?!]|\\n\\n+\")\n",
    "words_regex = re.compile(r\"[\\w']+\", re.U)\n",
    "special_chars = \".:,;?!-_\\\"'()„“”‚‘’…\"\n",
    "padding_token = \"_\" # This character can not occur in any of the words.\n",
    "k_tokens = 5 # How many tokens to base the next word on.\n",
    "tokenized_sentences = [] # Store your output in this list.\n",
    "# First split Hamlet into sentences, then tokenize each sentence.\n",
    "for sentence in sentence_regex.split(full):\n",
    "    pass # Your solution here\n",
    "ohd = OneHotDict(np.unique([token for sentence in tokenized_sentences for token in sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f1593",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_diy_word_embeddings.hidden_tests_33_0(ohd, padding_token, special_chars, tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436bcc4b",
   "metadata": {},
   "source": [
    "You can now train your model.\n",
    "The cell below contains an example model that can consume one-hot vectors and produce a probability distribution, which can be compared to another one-hot vector using the `CELoss`.\n",
    "The train loop comes with a floating loss computation to keep you updated on the training progress.\n",
    "You will still have to add the random selection of inputs and outputs and translate from words to one-hot vectors.\n",
    "Keep in mind that the `csr_array`s are always two-dimensional and matrix operations including them will typically return `csr_array`s again.\n",
    "To have it simple, you can call `toarray()` on a `csr_array` to turn it into a `numpy` array.\n",
    "For the input vectors, that is a bit slower but may be necessary unless you adapt your `Linear.backward` implementation to handle `csr_array` inputs.\n",
    "\n",
    "We can help the gradient descent with an additional constraint: Enforcing all embeddings vectors to be normalized.\n",
    "Simply normalize all row vectors of `enc_layers[0].weights` **in-place** after each `step_gradient` call.\n",
    "Normalizing the vectors will prevent explosion or vanishing of the vector norms, which may otherwise occur and lead to a bunch of `nan` values.\n",
    "\n",
    "In the browser, the training process will probably be horrendously slow, due to the limitation to one thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the embedding dimensionality\n",
    "word_vec_dim = 200\n",
    "# Creating the embedding encoder layers for each input token\n",
    "enc_layers = [Linear(len(ohd.vocabulary), word_vec_dim, use_bias=False)]\n",
    "for _ in range(k_tokens-1): enc_layers.append(LinkedLinear(enc_layers[0], use_bias=False))\n",
    "# Creating a linked decoder layer\n",
    "dec_layer = LinkedTransposedLinear(enc_layers[0], use_bias=False)\n",
    "# Creating the rest of the model\n",
    "model = ModuleChain([\n",
    "    ModuleConcat(enc_layers),\n",
    "    Linear(k_tokens*word_vec_dim, 2000),\n",
    "    ReLU(),\n",
    "    Linear(2000, word_vec_dim),\n",
    "    dec_layer,\n",
    "    Softmax()\n",
    "])\n",
    "loss = CELoss()\n",
    "floating_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting training parameters\n",
    "n_iterations = 5000\n",
    "learning_rate = 1e-1\n",
    "floating_factor = 0.999\n",
    "# Doing the actual training. You can run this cell multiple times to continue training\n",
    "with tqdm(range(n_iterations), total=n_iterations) as bar:\n",
    "    for _ in bar:\n",
    "        # Insert code for a single training iteration.\n",
    "        # Select a random sentence and a random range of k+1 tokens.\n",
    "        # The first k of these tokens are the input and the (k+1)-th token is the output.\n",
    "        # Translate all tokens into one-hot vectors and run forward.\n",
    "        # Calculate the loss_value for the floating loss visualization.\n",
    "        # Backpropagate the loss using backward and step_gradient.\n",
    "        pass # Your solution here\n",
    "        # loss_value = ...\n",
    "        floating_loss = loss_value if floating_loss is None else floating_factor * floating_loss + (1-floating_factor) * loss_value\n",
    "        bar.desc = f\"Floating loss: {floating_loss:>7.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8313f76",
   "metadata": {},
   "source": [
    "It is now time to generate text using your model.\n",
    "Start with $k$ times the `padding_token` to signify the sentence start.\n",
    "Then create the next word by computing the word probabilities using the $k$ last tokens in your sentence.\n",
    "Sample a word according to the word probabilities.\n",
    "You can do so by sampling a random float in $[0,1]$ and finding the $i$ most likeliest word for which the cumulative probability of the $i$ most likely words exceeds this float (nucleus sampling).\n",
    "Using the `OneHotDictionary` you can then add that word to the sentence.\n",
    "Print all generated words.\n",
    "\n",
    "Try different sampling techniques. Which works best from your point of view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "generated_words = []\n",
    "pass # Your solution here\n",
    "while (len(generated_words) == 0 or generated_words[-1] != padding_token) and len(generated_words) < 50:\n",
    "\t# Process:\n",
    "\t# - feed forward the last k words as one-hot vectors\n",
    "\t# - sample word according to predicted probabilities\n",
    "\t# - append word to sentence print the word\n",
    "\tpass # Your solution here\n",
    "\tgenerated_words.append(next_word)\n",
    "\tprint(next_word, end=\" \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e27fc",
   "metadata": {},
   "source": [
    "Analyze the resulting word embeddings a bit.\n",
    "What word vectors are most similar in total or relative to specific words?\n",
    "Are there relevant clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some stuff\n",
    "pass # Your solution here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
