{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "5f7d7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15111.84s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (2.2.5)\n",
      "Requirement already satisfied: numba in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (0.61.2)\n",
      "Requirement already satisfied: tqdm in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from numba) (0.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15117.51s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nats25-02-02-bpe==0.1\n",
      "  Downloading https://dm.cs.tu-dortmund.de/nats/nats25_02_02_bpe-0.1-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: nats25-02-02-bpe\n",
      "  Attempting uninstall: nats25-02-02-bpe\n",
      "    Found existing installation: nats25_02_02_bpe 0.1\n",
      "    Uninstalling nats25_02_02_bpe-0.1:\n",
      "      Successfully uninstalled nats25_02_02_bpe-0.1\n",
      "Successfully installed nats25-02-02-bpe-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy numba tqdm pandas\n",
    "%pip install --no-cache-dir --force-reinstall https://dm.cs.tu-dortmund.de/nats/nats25_02_02_bpe-0.1-py3-none-any.whl\n",
    "import nats25_02_02_bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe56cc",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding\n",
    "\n",
    "In this assignment, your task is to implement the training of a byte-pair-encoding tokenizer yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b9993be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, re\n",
    "from numba import jit\n",
    "try: from tqdm.notebook import tqdm # optional\n",
    "except: tqdm = None\n",
    "\n",
    "# Load the input data\n",
    "import gzip, json, urllib\n",
    "file_path, _ = urllib.request.urlretrieve(\"https://dm.cs.tu-dortmund.de/nats/data/minecraft-articles.json.gz\")\n",
    "raw = json.load(gzip.open(file_path, \"rt\", encoding=\"utf-8\"))\n",
    "titles, texts, classes = [x[\"title\"] for x in raw], [x[\"text\"] for x in raw], [x[\"heuristic\"] for x in raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74089f6",
   "metadata": {},
   "source": [
    "## Join texts into a single sequence of bytes.\n",
    "\n",
    "Split all the provided texts (`title` and `text`) using the given whitespace pretokenizer. Encode the tokens as bytes with UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d02c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714539\n"
     ]
    }
   ],
   "source": [
    "#in this we tokenized the titles and texts into one singular data array where each element is a byte represenation of each token \n",
    "pretokenizer=re.compile(r\"\\n|\\s*\\S+\")\n",
    "data = []\n",
    "for title in titles:\n",
    "    split_text = pretokenizer.findall(string=title)\n",
    "    for elm in split_text:\n",
    "        data.append(elm)\n",
    "\n",
    "for text in texts: \n",
    "    split_text = pretokenizer.findall(string=text)\n",
    "    for elm in split_text:\n",
    "        data.append(elm)\n",
    "for index in range(0,len(data)): \n",
    "    data[index] = data[index].encode(encoding=\"utf-8\")\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "4e4effe9",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_4_0(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "3bb38712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4649344,)\n"
     ]
    }
   ],
   "source": [
    "# In the following, we will use lists containing numpy arrays with int16\n",
    "data = np.array([int(x) for x in b\"\\0\".join(data)], dtype=np.int16)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "0ab2a8ec",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_6_0(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223075e",
   "metadata": {},
   "source": [
    "## Write a function to find the most common two symbols in a sequence\n",
    "\n",
    "While this will be the performance bottleneck of the implementation, you may use a `Counter` of pairs here.\n",
    "\n",
    "In our experiments, a vectorized numpy solution was 60x faster.\n",
    "\n",
    "- Return a pair of ints (we *will* exceed the byte range).\n",
    "- Skip 0 tokens used as separators\n",
    "- The second token must not be a space or newline (\"pre-tokenization\")\n",
    "- When no token occurs more than once, return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "87b0d593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 116\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def find_most_frequent(seq):\n",
    "    \"\"\"\n",
    "    Alternative vectorized solution using hash-based approach for integer sequences.\n",
    "    This is faster for integer sequences but limited to integer data types.\n",
    "    \n",
    "    Args:\n",
    "        seq: sequence/array of integers\n",
    "    \n",
    "    Returns:\n",
    "        tuple: most frequent pair or None if no pair occurs more than once\n",
    "    \"\"\"\n",
    "    seq = np.asarray(seq)\n",
    "    \n",
    "    if len(seq) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Create adjacent pairs\n",
    "    pairs_left = seq[:-1]\n",
    "    pairs_right = seq[1:]\n",
    "    \n",
    "    # Filter out pairs containing 0\n",
    "    mask = (pairs_left != 0) & (pairs_right != 0)\n",
    "    \n",
    "    if not np.any(mask):\n",
    "        return None\n",
    "    \n",
    "    valid_left = pairs_left[mask]\n",
    "    valid_right = pairs_right[mask]\n",
    "    \n",
    "    if len(valid_left) == 0:\n",
    "        return None\n",
    "    \n",
    "    # For integer sequences, we can use a hash-like approach\n",
    "    # Combine pairs into single numbers for faster counting\n",
    "    max_val = max(np.max(valid_left), np.max(valid_right))\n",
    "    if max_val < 2**16:  # Safe for combining into 32-bit int\n",
    "        combined = valid_left.astype(np.int32) * (max_val + 1) + valid_right.astype(np.int32)\n",
    "        unique_combined, counts = np.unique(combined, return_counts=True)\n",
    "        \n",
    "        max_count_idx = np.argmax(counts)\n",
    "        max_count = counts[max_count_idx]\n",
    "        \n",
    "        if max_count > 1:\n",
    "            most_frequent_combined = unique_combined[max_count_idx]\n",
    "            left = most_frequent_combined // (max_val + 1)\n",
    "            right = most_frequent_combined % (max_val + 1)\n",
    "            return (np.int16(left), np.int16(right))\n",
    "    \n",
    "    return None\n",
    "'''def find_most_frequent(seq):\n",
    "    # We create pairs of adjacent tokens from the sequence.\n",
    "    # We filter out any pairs that contain the '0' separator token,\n",
    "    # as instructed.\n",
    "    pairs = (\n",
    "        (seq[i], seq[i+1]) \n",
    "        for i in range(len(seq) - 1) \n",
    "        if seq[i] != 0 and seq[i+1] != 0\n",
    "    )\n",
    "    \n",
    "    # We use Counter to find the most common pair.\n",
    "    pair_counts = Counter(pairs)\n",
    "    \n",
    "    # If the counter is empty or no pair occurs more than once, return None.\n",
    "    if not pair_counts:\n",
    "        return None\n",
    "        \n",
    "    most_frequent_pair, count = pair_counts.most_common(1)[0]\n",
    "    \n",
    "    if count > 1:\n",
    "        return most_frequent_pair\n",
    "    else:\n",
    "        return None\n",
    "'''\n",
    "'''def find_most_frequent1(seq):\n",
    "    df = pd.DataFrame(seq, columns=['value'])\n",
    "    df = df[df.value != 0]\n",
    "    seq = df.to_numpy()\n",
    "    seq = seq.squeeze()\n",
    "    c = Counter(seq)\n",
    "    most_freq = c.most_common(n=2)\n",
    "    print(\"most freq element: \", most_freq[0][0])\n",
    "    return most_freq[0][0], most_freq[1][0]\n",
    "'''\n",
    "result = find_most_frequent(seq = data)\n",
    "x,y = result \n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de143b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "1cddb65e",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_9_0(find_most_frequent, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc7662",
   "metadata": {},
   "source": [
    "## Initialize the vocabulary\n",
    "\n",
    "Our initial vocabulary contains all 256 bytes, so we can later still encode any character (or byte sequence) not in our training data.\n",
    "The vocabulary is used for decoding, so it is a map from integer token ids to bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f853418c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff'}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_vocab():\n",
    "    #Encode all of the integers 0 to 255 to their byte representations\n",
    "    #the integers in this case represent the ascii code for the symbol \n",
    "    vocab = dict()\n",
    "    for i in range(0,256): \n",
    "        vocab[i] = bytes([i])\n",
    "    return vocab\n",
    "init_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "76b58a77",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_12_0(init_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bbaf7",
   "metadata": {},
   "source": [
    "## Token replacement function\n",
    "\n",
    "In the given sequence, replace tokens (a,b) with a new token c. Avoid copying, but modify the sequence in-place. You can use `numba.jit` to make this (much) faster.\n",
    "\n",
    "Return the resulting array (-view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "6de68c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def replace(seq, a, b, c):\n",
    "    \"\"\"\n",
    "    A highly optimized Numba version that avoids list appends by using a \n",
    "    pre-allocated NumPy array.\n",
    "    \"\"\"\n",
    "    # Create a result array of the same type and maximum possible size.\n",
    "    new_seq = np.empty_like(seq)\n",
    "    \n",
    "    # 'i' will be the read-index for the input sequence 'seq'\n",
    "    # 'j' will be the write-index for the output sequence 'new_seq'\n",
    "    i = 0\n",
    "    j = 0 \n",
    "    \n",
    "    while i < len(seq):\n",
    "        if i + 1 < len(seq) and seq[i] == a and seq[i+1] == b:\n",
    "            new_seq[j] = c\n",
    "            i += 2 # Advance read-index by 2\n",
    "            j += 1 # Advance write-index by 1\n",
    "        else:\n",
    "            new_seq[j] = seq[i]\n",
    "            i += 1 # Advance read-index by 1\n",
    "            j += 1 # Advance write-index by 1\n",
    "    \n",
    "    # The final result is the slice of the array that we actually used.\n",
    "    # This trimming operation is very fast.\n",
    "    return new_seq[:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "3f3c88fa",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_15_0(replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449f21d",
   "metadata": {},
   "source": [
    "## Train BPE\n",
    "\n",
    "Implement a function to train a byte-pair encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "443ff8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(indata, size=1000):\n",
    "    merges = list() # of tuples(id1, id2)\n",
    "    new_id = 256\n",
    "    vocab = init_vocab()\n",
    "    data = np.array(indata, dtype=np.int16) # copy to allow modifications\n",
    "    print(\"length of data: \", len(data))\n",
    "    print(data)\n",
    "    #scan the text to find the most common pairs \n",
    "    while len(vocab) < size: \n",
    "        pair = find_most_frequent(data)\n",
    "        data = replace(data, pair[0], pair[1], new_id)\n",
    "        new_id = len(vocab)\n",
    "        #add the mapping such that it can be reversed (decoded)\n",
    "        vocab[new_id] = vocab[pair[0]] + vocab[pair[1]]\n",
    "        merges.append((pair,new_id))\n",
    "    print(vocab)\n",
    "    print(\"Compression factor:\", len(data) / len(indata))\n",
    "    return vocab, merges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "6e29e01d",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data:  29\n",
      "[ 72 101 108 108 111  32 119 111 114 108 100  44  32 104 101 108 108 111\n",
      "  32 101 118 101 114 121  98 111 100 121  46]\n",
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'el', 257: b'lo', 258: b'el '}\n",
      "Compression factor: 0.7931034482758621\n"
     ]
    }
   ],
   "source": [
    "nats25_02_02_bpe.hidden_tests_18_0(train_bpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00ec66",
   "metadata": {},
   "source": [
    "## Train a tokenizer on our training data\n",
    "\n",
    "Inspect the longest tokens generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "62fc3547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data:  4649344\n",
      "[ 83 101 114 ... 109 111 114]\n",
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b' t', 257: b' a', 258: b'he', 259: b'in', 260: b'er', 261: b' t a', 262: b'on', 263: b' b', 264: b're', 265: b' s', 266: b' c', 267: b'ed', 268: b'it', 269: b' o', 270: b'at', 271: b'an', 272: b'en', 273: b' p', 274: b' w', 275: b'heg', 276: b'es', 277: b' f', 278: b'or', 279: b'is', 280: b' tn', 281: b' d', 282: b'la', 283: b'lo', 284: b'le', 285: b'ro', 286: b'ck', 287: b' he', 288: b' m', 289: b'al', 290: b' to', 291: b'itf', 292: b'ar', 293: b'isd', 294: b'i t a', 295: b'as', 296: b' n', 297: b'one', 298: b' or', 299: b'us', 300: b'ot', 301: b' h', 302: b'ant', 303: b'ra', 304: b' g', 305: b've', 306: b'  b', 307: b'ec', 308: b' e', 309: b'un', 310: b'laro', 311: b'om', 312: b' ed', 313: b'il', 314: b'  t a', 315: b'am', 316: b' th', 317: b'ly', 318: b'st', 319: b'ce', 320: b'onun', 321: b'ch', 322: b' dy', 323: b' sat', 324: b'ig', 325: b'et', 326: b'T a', 327: b' ped', 328: b'ret', 329: b'ins', 330: b'itr', 331: b'T ah', 332: b'ut', 333: b' v', 334: b'ur', 335: b'id', 336: b' ar', 337: b'ol', 338: b'ct', 339: b'ul', 340: b' heo', 341: b'im', 342: b' t b', 343: b'ench', 344: b'es f', 345: b'ony', 346: b's.', 347: b'ow', 348: b's,', 349: b' tl', 350: b'em', 351: b' ts', 352: b' 1', 353: b'ir', 354: b'ic', 355: b'ft', 356: b' (', 357: b'rep', 358: b'ge', 359: b'if', 360: b' M', 361: b' l', 362: b'  or', 363: b'pp', 364: b'am o', 365: b'atd', 366: b' t bin', 367: b' et', 368: b' S', 369: b'vin', 370: b'  t ae', 371: b' oe', 372: b'wn', 373: b'ad', 374: b' I', 375: b' sh', 376: b'ta', 377: b' 2', 378: b' E', 379: b'el', 380: b'antic', 381: b'asus', 382: b' an', 383: b' tt', 384: b' oisd', 385: b'up', 386: b'lem', 387: b' A', 388: b'law', 389: b'he  b', 390: b' tr', 391: b'att', 392: b' ed', 393: b' slaro', 394: b'um', 395: b'ke', 396: b' hea', 397: b'esup', 398: b'  bt', 399: b' ph', 400: b'blo', 401: b' lo', 402: b'ecx', 403: b'edisd', 404: b' sath', 405: b' t ae', 406: b'u b', 407: b' \"', 408: b' T', 409: b' s t a', 410: b'ces', 411: b'a oe', 412: b'so', 413: b'i b', 414: b' B', 415: b'om tl', 416: b' an', 417: b'lawel', 418: b'iv', 419: b' tne', 420: b'edisdt', 421: b'iro', 422: b'itft', 423: b' C', 424: b'ex', 425: b'decx', 426: b' edl', 427: b' gl', 428: b' P', 429: b'  oisd', 430: b'ort', 431: b' ed d', 432: b'av', 433: b'arep', 434: b'qu', 435: b'  bti', 436: b'se', 437: b' r', 438: b'a g', 439: b' Ct', 440: b' tno', 441: b'ug', 442: b' pi t a', 443: b' pasus', 444: b' m th', 445: b' o c', 446: b'anin', 447: b'i g', 448: b'edy', 449: b' so', 450: b' (ces', 451: b'ss', 452: b'ld', 453: b' ml', 454: b'tid', 455: b'ulb', 456: b'm h', 457: b'ff', 458: b'ay', 459: b'ckt', 460: b' oin', 461: b'iheg', 462: b'itfd', 463: b'reh', 464: b'lep', 465: b\"'s\", 466: b'oo', 467: b'd w', 468: b'pt', 469: b' ge', 470: b'ery', 471: b' \\n', 472: b' 2ex', 473: b' N', 474: b'de', 475: b'ra o c', 476: b'ot r', 477: b'0.', 478: b'eitf', 479: b'ortg', 480: b' ft', 481: b'hegs', 482: b' L', 483: b'if an', 484: b'  a', 485: b' shhe', 486: b'atst', 487: b'lew', 488: b' td', 489: b'ta0', 490: b's,a oe', 491: b'onT ah', 492: b' satn', 493: b'hegt', 494: b'isy', 495: b'ak', 496: b'ctt', 497: b'eno', 498: b'en d', 499: b' c.', 500: b'  d', 501: b'th', 502: b' fss', 503: b' la', 504: b' F', 505: b'atg', 506: b' W', 507: b'din', 508: b'reo', 509: b'ure', 510: b'aro', 511: b'oti t a'}\n",
      "Compression factor: 0.5716608622635796\n",
      "CPU times: user 15 s, sys: 7.65 ms, total: 15 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab, merges = train_bpe(data, 512) # begin with 512 â€“ at 1024, we get many more words as standalone tokens, but the runtime increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "272a8695",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No token for ' block'?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAssertionError\u001b[39m: No token for ' block'?"
     ]
    }
   ],
   "source": [
    "nats25_02_02_bpe.hidden_tests_21_0(vocab, merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724064bb",
   "metadata": {},
   "source": [
    "## Tokenization function\n",
    "\n",
    "Implement a function to tokenize a string given the vocabulary and merges.\n",
    "\n",
    "While not the most efficient, it is fine to implement this using `replace` above. To improve performance, call `replace` only when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80811ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "def tokenize(merges, s):\n",
    "    #in this we tokenized the titles and texts into one singular data array where each element is a byte represenation of each token \n",
    "    pretokenizer=re.compile(r\"\\n|\\s*\\S+\")\n",
    "    data = []\n",
    "    for words in s:\n",
    "        split_text = pretokenizer.findall(string=title)\n",
    "        for elm in split_text:\n",
    "            data.append(elm)\n",
    "    for index in range(0,len(data)): \n",
    "        data[index] = data[index].encode(encoding=\"utf-8\")\n",
    "    print(len(data))\n",
    "    # In the following, we will use lists containing numpy arrays with int16\n",
    "    data = np.array([int(x) for x in b\"\\0\".join(data)], dtype=np.int16)\n",
    "    while len(vocab) < size: \n",
    "        pair = find_most_frequent(data)\n",
    "        data = replace(data, pair[0], pair[1], new_id)\n",
    "        new_id = len(vocab)\n",
    "        #add the mapping such that it can be reversed (decoded)\n",
    "        vocab[new_id] = vocab[pair[0]] + vocab[pair[1]]\n",
    "        merges.append((pair,new_id))\n",
    "\n",
    "s_tmp = tokenize(merges, \"The data set is about Minecraft.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "2eee2d8b",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[31mTypeError\u001b[39m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "nats25_02_02_bpe.hidden_tests_24_0(vocab, merges, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2e08b",
   "metadata": {},
   "source": [
    "## Decoding function\n",
    "\n",
    "Implement a function to decode a token sequence into a regular string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6455b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(vocab, tokens):\n",
    "    s = None\n",
    "    pass # Your solution here\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc142d6",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_27_0(vocab, merges, tokenize, decode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prak_opt_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
