{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy numba tqdm pandas\n",
    "%pip install --no-cache-dir --force-reinstall https://dm.cs.tu-dortmund.de/nats/nats25_02_02_bpe-0.1-py3-none-any.whl\n",
    "import nats25_02_02_bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe56cc",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding\n",
    "\n",
    "In this assignment, your task is to implement the training of a byte-pair-encoding tokenizer yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9993be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, re\n",
    "from numba import jit\n",
    "try: from tqdm.notebook import tqdm # optional\n",
    "except: tqdm = None\n",
    "\n",
    "# Load the input data\n",
    "import gzip, json, urllib\n",
    "file_path, _ = urllib.request.urlretrieve(\"https://dm.cs.tu-dortmund.de/nats/data/minecraft-articles.json.gz\")\n",
    "raw = json.load(gzip.open(file_path, \"rt\", encoding=\"utf-8\"))\n",
    "titles, texts, classes = [x[\"title\"] for x in raw], [x[\"text\"] for x in raw], [x[\"heuristic\"] for x in raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74089f6",
   "metadata": {},
   "source": [
    "## Join texts into a single sequence of bytes.\n",
    "\n",
    "Split all the provided texts (`title` and `text`) using the given whitespace pretokenizer. Encode the tokens as bytes with UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenizer=re.compile(r\"\\n|\\s*\\S+\")\n",
    "data = None # concatenated data\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4effe9",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_4_0(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb38712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following, we will use lists containing numpy arrays with int16\n",
    "data = np.array([int(x) for x in b\"\\0\".join(data)], dtype=np.int16)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2a8ec",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_6_0(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223075e",
   "metadata": {},
   "source": [
    "## Write a function to find the most common two symbols in a sequence\n",
    "\n",
    "While this will be the performance bottleneck of the implementation, you may use a `Counter` of pairs here.\n",
    "\n",
    "In our experiments, a vectorized numpy solution was 60x faster.\n",
    "\n",
    "- Return a pair of ints (we *will* exceed the byte range).\n",
    "- Skip 0 tokens used as separators\n",
    "- The second token must not be a space or newline (\"pre-tokenization\")\n",
    "- When no token occurs more than once, return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def find_most_frequent(seq):\n",
    "    pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cddb65e",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_9_0(find_most_frequent, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc7662",
   "metadata": {},
   "source": [
    "## Initialize the vocabulary\n",
    "\n",
    "Our initial vocabulary contains all 256 bytes, so we can later still encode any character (or byte sequence) not in our training data.\n",
    "The vocabulary is used for decoding, so it is a map from integer token ids to bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vocab():\n",
    "    vocab = dict() # int to bytes\n",
    "    pass # Your solution here\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b58a77",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_12_0(init_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bbaf7",
   "metadata": {},
   "source": [
    "## Token replacement function\n",
    "\n",
    "In the given sequence, replace tokens (a,b) with a new token c. Avoid copying, but modify the sequence in-place. You can use `numba.jit` to make this (much) faster.\n",
    "\n",
    "Return the resulting array (-view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de68c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(seq, a, b, c):\n",
    "    pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c88fa",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_15_0(replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449f21d",
   "metadata": {},
   "source": [
    "## Train BPE\n",
    "\n",
    "Implement a function to train a byte-pair encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ff8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(indata, size=1000):\n",
    "    merges = list() # of tuples(id1, id2)\n",
    "    vocab = init_vocab()\n",
    "    data = np.array(indata, dtype=np.int16) # copy to allow modifications\n",
    "    pbar = tqdm(total=size-256) if tqdm else None # optional\n",
    "\n",
    "    pass # Your solution here\n",
    "\n",
    "    if pbar: pbar.close() # finish progressbar\n",
    "    print(\"Compression factor:\", len(data) / len(indata))\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29e01d",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_18_0(train_bpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00ec66",
   "metadata": {},
   "source": [
    "## Train a tokenizer on our training data\n",
    "\n",
    "Inspect the longest tokens generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vocab, merges = train_bpe(data, 1024) # begin with 512 â€“ at 1024, we get many more words as standalone tokens, but the runtime increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a8695",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_21_0(vocab, merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724064bb",
   "metadata": {},
   "source": [
    "## Tokenization function\n",
    "\n",
    "Implement a function to tokenize a string given the vocabulary and merges.\n",
    "\n",
    "While not the most efficient, it is fine to implement this using `replace` above. To improve performance, call `replace` only when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80811ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(merges, s):\n",
    "    tokens = None # np.array of int16 as above\n",
    "    pass # Your solution here\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee2d8b",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_24_0(vocab, merges, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2e08b",
   "metadata": {},
   "source": [
    "## Decoding function\n",
    "\n",
    "Implement a function to decode a token sequence into a regular string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6455b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(vocab, tokens):\n",
    "    s = None\n",
    "    pass # Your solution here\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc142d6",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_02_02_bpe.hidden_tests_27_0(vocab, merges, tokenize, decode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
