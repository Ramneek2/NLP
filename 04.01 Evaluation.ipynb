{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f30f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (2.2.5)\n",
      "Requirement already satisfied: scikit-learn in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ramneek/miniconda3/envs/prak_opt_env_conda/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nats25-04-01-evaluation==0.1\n",
      "  Downloading https://dm.cs.tu-dortmund.de/nats/nats25_04_01_evaluation-0.1-py3-none-any.whl (3.3 kB)\n",
      "Installing collected packages: nats25-04-01-evaluation\n",
      "  Attempting uninstall: nats25-04-01-evaluation\n",
      "    Found existing installation: nats25_04_01_evaluation 0.1\n",
      "    Uninstalling nats25_04_01_evaluation-0.1:\n",
      "      Successfully uninstalled nats25_04_01_evaluation-0.1\n",
      "Successfully installed nats25-04-01-evaluation-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy scikit-learn pandas\n",
    "%pip install --no-cache-dir --force-reinstall https://dm.cs.tu-dortmund.de/nats/nats25_04_01_evaluation-0.1-py3-none-any.whl\n",
    "import nats25_04_01_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99670993",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In this (shorter) assignment, we want to compare the quality of different clustering approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b39733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Franchise', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Franchise', 'Franchise', 'Blocks', 'Blocks', 'Blocks', 'Mobs', 'Mobs', 'Mobs', 'Items', 'Mobs', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Mobs', 'Blocks', 'Franchise', 'Blocks', 'Gameplay', 'Mojang', 'Blocks', 'Blocks', 'Mojang', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Items', 'Blocks', 'Items', 'Items', 'Blocks', 'Franchise', 'Versions', 'Franchise', 'Items', 'Items', 'Blocks', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Gameplay', 'Gameplay', 'Mobs', 'Items', 'Items', 'Items', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Gameplay', 'Gameplay', 'Blocks', 'Gameplay', 'Gameplay', 'Blocks', 'Gameplay', 'Gameplay', 'Gameplay', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Blocks', 'Blocks', 'Mobs', 'Items', 'Items', 'Items', 'Blocks', 'Items', 'Blocks', 'Items', 'Blocks', 'Gameplay', 'Gameplay', 'Mobs', 'Items', 'Mobs', 'Gameplay', 'Mojang', 'Blocks', 'Items', 'Gameplay', 'Mojang', 'Franchise', 'Items', 'Gameplay', 'Mobs', 'Items', 'Items', 'Items', 'Franchise', 'Mojang', 'Mojang', 'Versions', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Franchise', 'Franchise', 'Mobs', 'Gameplay', 'Franchise', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Mobs', 'Items', 'Items', 'Mobs', 'Mobs', 'Mobs', 'Mojang', 'Mobs', 'Gameplay', 'Gameplay', 'Gameplay', 'Blocks', 'Franchise', 'Gameplay', 'Items', 'Items', 'Franchise', 'Blocks', 'Items', 'Mojang', 'Gameplay', 'Gameplay', 'Gameplay', 'Items', 'Items', 'Mobs', 'Items', 'Items', 'Items', 'Items', 'Blocks', 'Blocks', 'Gameplay', 'Items', 'Items', 'Blocks', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Gameplay', 'Mojang', 'Franchise', 'Mojang', 'Mobs', 'Mojang', 'Mojang', 'Franchise', 'Franchise', 'Blocks', 'Blocks', 'Mobs', 'Gameplay', 'Mobs', 'Gameplay', 'Items', 'Gameplay', 'Blocks', 'Blocks', 'Items', 'Mojang', 'Franchise', 'Gameplay', 'Gameplay', 'Blocks', 'Versions', 'Blocks', 'Mojang', 'Gameplay', 'Characters', 'Items', 'Blocks', 'Mojang', 'Gameplay', 'Blocks', 'Versions', 'Mobs', 'Blocks', 'Blocks', 'Franchise', 'Mobs', 'Blocks', 'Versions', 'Items', 'Items', 'Franchise', 'Blocks', 'Items', 'Versions', 'Franchise', 'Mobs', 'Versions', 'Items', 'Versions', 'Mojang', 'Gameplay', 'Mojang', 'Items', 'Mojang', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Mobs', 'Blocks', 'Gameplay', 'Versions', 'Gameplay', 'Blocks', 'Mobs', 'Gameplay', 'Mobs', 'Items', 'Items', 'Blocks', 'Items', 'Mobs', 'Mobs', 'Mobs', 'Blocks', 'Blocks', 'Gameplay', 'Items', 'Mojang', 'Items', 'Items', 'Items', 'Items', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Mobs', 'Gameplay', 'Gameplay', 'Mobs', 'Blocks', 'Mojang', 'Items', 'Blocks', 'Gameplay', 'Gameplay', 'Gameplay', 'Gameplay', 'Gameplay', 'Blocks', 'Gameplay', 'Gameplay', 'Gameplay', 'Gameplay', 'Blocks', 'Blocks', 'Franchise', 'Mobs', 'Gameplay', 'Mojang', 'Items', 'Mobs', 'Franchise', 'Franchise', 'Versions', 'Blocks', 'Versions', 'Gameplay', 'Versions', 'Items', 'Items', 'Blocks', 'Blocks', 'Gameplay', 'Gameplay', 'Blocks', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Gameplay', 'Mojang', 'Mojang', 'Franchise', 'Versions', 'Versions', 'Mojang', 'Gameplay', 'Blocks', 'Mojang', 'Blocks', 'Mobs', 'Mojang', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Mobs', 'Items', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Versions', 'Items', 'Mobs', 'Items', 'Mobs', 'Mobs', 'Franchise', 'Blocks', 'Gameplay', 'Blocks', 'Blocks', 'Versions', 'Versions', 'Gameplay', 'Items', 'Items', 'Blocks', 'Blocks', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Items', 'Franchise', 'Gameplay', 'Gameplay', 'Blocks', 'Blocks', 'Gameplay', 'Blocks', 'Blocks', 'Blocks', 'Mobs', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Mojang', 'Blocks', 'Mojang', 'Blocks', 'Gameplay', 'Mojang', 'Franchise', 'Gameplay', 'Items', 'Mobs', 'Mojang', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Blocks', 'Items', 'Items', 'Blocks', 'Blocks', 'Mobs', 'Items', 'Blocks', 'Items', 'Items', 'Blocks', 'Blocks', 'Gameplay', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Blocks', 'Versions', 'Mojang', 'Mojang', 'Blocks', 'Blocks', 'Items', 'Items', 'Mobs', 'Gameplay', 'Mobs', 'Blocks', 'Versions', 'Blocks', 'Blocks', 'Items', 'Franchise', 'Blocks', 'Mobs', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Mobs', 'Franchise', 'Gameplay', 'Mojang', 'Gameplay', 'Mobs', 'Blocks', 'Mobs', 'Blocks', 'Mobs', 'Blocks', 'Blocks', 'Items', 'Items', 'Mobs', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Mobs', 'Blocks', 'Franchise', 'Versions', 'Blocks', 'Franchise', 'Versions', 'Blocks', 'Blocks', 'Mobs', 'Gameplay', 'Blocks', 'Franchise', 'Blocks', 'Franchise', 'Versions', 'Franchise', 'Blocks', 'Blocks', 'Franchise', 'Franchise', 'Mobs', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Gameplay', 'Blocks', 'Mojang', 'Blocks', 'Blocks', 'Items', 'Items', 'Items', 'Versions', 'Mobs', 'Franchise', 'Gameplay', 'Blocks', 'Blocks', 'Mojang', 'Blocks', 'Versions', 'Mojang', 'Mobs', 'Mobs', 'Mobs', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Mojang', 'Gameplay', 'Blocks', 'Versions', 'Versions', 'Blocks', 'Franchise', 'Versions', 'Versions', 'Gameplay', 'Versions', 'Blocks', 'Items', 'Mobs', 'Mobs', 'Mobs', 'Mobs', 'Mobs', 'Versions', 'Versions', 'Versions', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Franchise', 'Versions', 'Mobs', 'Mobs', 'Versions', 'Gameplay', 'Franchise', 'Mobs', 'Franchise', 'Versions', 'Mojang', 'Mojang', 'Franchise', 'Franchise', 'Versions', 'Versions', 'Mojang', 'Franchise', 'Versions', 'Versions', 'Franchise', 'Franchise', 'Versions', 'Mobs', 'Blocks', 'Gameplay', 'Versions', 'Blocks', 'Franchise', 'Versions', 'Mobs', 'Items', 'Franchise', 'Mobs', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Items', 'Mobs', 'Mobs', 'Mobs', 'Franchise', 'Blocks', 'Blocks', 'Items', 'Items', 'Blocks', 'Blocks', 'Mojang', 'Items', 'Gameplay', 'Gameplay', 'Blocks', 'Blocks', 'Blocks', 'Versions', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Versions', 'Versions', 'Gameplay', 'Versions', 'Versions', 'Gameplay', 'Blocks', 'Gameplay', 'Franchise', 'Versions', 'Gameplay', 'Gameplay', 'Versions', 'Gameplay', 'Mojang', 'Franchise', 'Items', 'Items', 'Items', 'Franchise', 'Versions', 'Mobs', 'Mobs', 'Mobs', 'Mobs', 'Items', 'Blocks', 'Blocks', 'Items', 'Items', 'Items', 'Items', 'Items', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Versions', 'Blocks', 'Blocks', 'Gameplay', 'Mojang', 'Mojang', 'Mobs', 'Versions', 'Blocks', 'Mobs', 'Mobs', 'Mobs', 'Mobs', 'Franchise', 'Items', 'Mojang', 'Franchise', 'Franchise', 'Blocks', 'Versions', 'Franchise', 'Mojang', 'Franchise', 'Mobs', 'Versions', 'Gameplay', 'Versions', 'Versions', 'Gameplay', 'Franchise', 'Mojang', 'Mojang', 'Blocks', 'Franchise', 'Mobs', 'Items', 'Items', 'Blocks', 'Blocks', 'Franchise', 'Franchise', 'Mobs', 'Mojang', 'Mobs', 'Blocks', 'Mobs', 'Versions', 'Gameplay', 'Blocks', 'Blocks', 'Mobs', 'Franchise', 'Items', 'Versions', 'Blocks', 'Blocks', 'Mojang', 'Versions', 'Gameplay', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Blocks', 'Gameplay', 'Gameplay', 'Gameplay', 'Blocks', 'Blocks', 'Blocks', 'Mobs', 'Franchise', 'Blocks', 'Mobs', 'Items', 'Gameplay', 'Mojang', 'Mojang', 'Mojang', 'Mobs', 'Franchise', 'Mobs', 'Franchise', 'Gameplay', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Versions', 'Franchise', 'Gameplay', 'Mojang', 'Franchise', 'Mobs', 'Blocks', 'Mojang', 'Gameplay', 'Mojang', 'Gameplay', 'Blocks', 'Mobs', 'Franchise', 'Gameplay', 'Franchise', 'Mojang', 'Mojang', 'Blocks', 'Versions', 'Versions', 'Blocks', 'Mobs', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Franchise', 'Items', 'Franchise', 'Blocks', 'Blocks', 'Franchise', 'Mobs', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Franchise', 'Versions', 'Items', 'Items', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Franchise', 'Mojang', 'Blocks', 'Gameplay', 'Franchise', 'Blocks', 'Gameplay', 'Versions', 'Versions', 'Gameplay', 'Versions', 'Franchise', 'Blocks', 'Blocks', 'Franchise', 'Franchise', 'Franchise', 'Blocks', 'Items', 'Versions', 'Versions', 'Versions', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Mobs', 'Mobs', 'Blocks', 'Versions', 'Versions', 'Versions', 'Franchise', 'Gameplay', 'Mojang', 'Mojang', 'Franchise', 'Gameplay', 'Gameplay', 'Gameplay', 'Versions', 'Blocks', 'Blocks', 'Blocks', 'Franchise', 'Gameplay', 'Versions', 'Versions', 'Franchise', 'Gameplay', 'Gameplay', 'Franchise', 'Franchise', 'Franchise', 'Franchise', 'Mojang', 'Mobs', 'Mobs', 'Gameplay', 'Blocks', 'Blocks', 'Versions', 'Franchise', 'Franchise', 'Mobs', 'Franchise', 'Versions', 'Blocks', 'Mobs', 'Franchise', 'Characters', 'Franchise', 'Characters', 'Characters', 'Characters', 'Characters', 'Characters', 'Characters', 'Characters', 'Characters', 'Franchise', 'Characters', 'Characters', 'Characters', 'Characters', 'Characters', 'Characters', 'Characters', 'Characters', 'Blocks', 'Characters', 'Characters', 'Characters', 'Characters', 'Franchise', 'Blocks', 'Blocks', 'Gameplay', 'Versions', 'Gameplay', 'Blocks', 'Characters', 'Versions', 'Versions', 'Gameplay', 'Items', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Gameplay', 'Items', 'Mojang', 'Gameplay', 'Franchise', 'Franchise', 'Franchise', 'Gameplay', 'Versions', 'Versions', 'Gameplay', 'Gameplay', 'Mobs', 'Gameplay', 'Blocks', 'Mobs', 'Blocks', 'Versions', 'Versions', 'Blocks', 'Blocks', 'Versions', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Blocks', 'Versions', 'Franchise', 'Blocks', 'Blocks', 'Blocks', 'Items', 'Mojang', 'Mobs', 'Mojang', 'Mojang', 'Items', 'Blocks', 'Blocks', 'Mojang', 'Mojang', 'Mojang', 'Mojang', 'Mojang', 'Mobs', 'Blocks', 'Gameplay', 'Mojang', 'Franchise', 'Franchise', 'Franchise', 'Franchise', 'Franchise', 'Franchise', 'Franchise', 'Gameplay', 'Franchise', 'Franchise', 'Franchise', 'Characters', 'Franchise', 'Mobs', 'Franchise', 'Mojang', 'Franchise', 'Mobs', 'Versions', 'Mobs', 'Gameplay', 'Versions', 'Gameplay', 'Versions', 'Items', 'Gameplay', 'Gameplay', 'Blocks', 'Blocks', 'Mobs', 'Versions', 'Gameplay', 'Gameplay', 'Gameplay', 'Gameplay', 'Gameplay', 'Items', 'Versions', 'Versions', 'Items', 'Franchise', 'Versions', 'Blocks', 'Versions', 'Franchise', 'Franchise']\n",
      "1026\n",
      "Blocks\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Load the input data\n",
    "import json, gzip, urllib\n",
    "file_path, _ = urllib.request.urlretrieve(\"https://dm.cs.tu-dortmund.de/nats/data/minecraft-articles.json.gz\")\n",
    "raw = json.load(gzip.open(file_path, \"rt\", encoding=\"utf-8\"))\n",
    "titles, texts, classes = [x[\"title\"] for x in raw], [x[\"text\"] for x in raw], [x[\"heuristic\"] for x in raw]\n",
    "print(classes)\n",
    "#the texts are assigned the to specific classes of texts \n",
    "print(len(texts))\n",
    "print(classes[2])\n",
    "#for example here the class[2] is blocks and the text is about a block\n",
    "#what we do now with clustering that we assign them to specific clusters\n",
    "# Vector 1 (Cluster Assignments): clusters = [0, 1, 0, 2, 1]\n",
    "# Vector 2 (True Classes): classes = ['Sports', 'Tech', 'Sports', 'Politics', 'Tech']\n",
    "# What we get after is a cluster assignment list for the documents  and a class list for the documents, both lists have the same size of the amount of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64025797",
   "metadata": {},
   "source": [
    "This is a minimal example implementation of spherical k-means, which we will use in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b43b65ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1026"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the text for k-means (minimalistic)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(stop_words=\"english\", sublinear_tf=True, smooth_idf=False, min_df=5)\n",
    "vect.fit(texts)\n",
    "vect.idf_ -= 1\n",
    "tfidf, idf = vect.transform(texts), vect.idf_\n",
    "vocabulary = vect.get_feature_names_out()\n",
    "tfidf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e146233",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your spherical-k-means implementation from the previous assignment here!\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def new_centers(tfidf, assignment):\n",
    "    \"\"\"\n",
    "    Calculates the new cluster centers in a more optimized way using vectorization.\n",
    "    \"\"\"\n",
    "    assignment = np.array(assignment)\n",
    "    if assignment.size == 0:\n",
    "        return np.array([])\n",
    "    k = np.int16(np.max(assignment) + 1)\n",
    "    \n",
    "    centers = []\n",
    "\n",
    "    for cluster_id in range(k):\n",
    "        indices_of_cluster_points = np.where(assignment == cluster_id)[0]\n",
    "\n",
    "        if len(indices_of_cluster_points) > 0:\n",
    "            cluster_points = tfidf[indices_of_cluster_points]\n",
    "            new_center = cluster_points.mean(axis=0)\n",
    "            normalized_center = normalize(np.asarray(new_center), norm='l2', axis=1)[0]\n",
    "            centers.append(normalized_center)\n",
    "        else:\n",
    "            centers.append(np.zeros(tfidf.shape[1]))\n",
    "            \n",
    "    return np.array(centers)\n",
    "\n",
    "def quality(tfidf, centers, assignment):\n",
    "    unique_center_ids = list(set(assignment))\n",
    "    #remember k * v array\n",
    "    #how to calculate the centers => k determines how many centers\n",
    "    #new centers by mean of each tfidf entry assigned to the cluster\n",
    "    output = 0\n",
    "    for unique_id in unique_center_ids: \n",
    "        sum_of_cosine_similarity = 0\n",
    "        for index,center_id in enumerate(assignment):\n",
    "            if unique_id == center_id: \n",
    "                doc_vector = tfidf[index, :].toarray()[0]\n",
    "                sum_of_cosine_similarity += doc_vector @ np.transpose(centers[center_id])\n",
    "        output += sum_of_cosine_similarity\n",
    "    return output    \n",
    "\n",
    "def reassign(tfidf, centers):\n",
    "    \"\"\"Reassign each object in tfidf to the most similar center.\n",
    "       Return a flat array, not a matrix.\"\"\"\n",
    "    \n",
    "    # This correctly calculates the cosine similarities in a vectorized way.\n",
    "    # The result is a sparse matrix.\n",
    "    similarities = tfidf @ centers.T\n",
    "    \n",
    "    # np.argmax on a sparse matrix returns a numpy.matrix of shape (n_docs, 1).\n",
    "    assignment_matrix = np.argmax(similarities, axis=1)\n",
    "    \n",
    "    # Convert the numpy.matrix to a flattened 1D numpy.ndarray to pass the tests.\n",
    "    # The .A1 attribute is a convenient shortcut for this.\n",
    "    if type(assignment_matrix) is np.matrix:\n",
    "        assignment_array = assignment_matrix.A1\n",
    "    else: \n",
    "        assignment_array = assignment_matrix\n",
    "    return assignment_array\n",
    "\n",
    "\n",
    "def initial_centers(tfidf, k, seed):\n",
    "    \"\"\"Choose k initial cluster centers.\"\"\"\n",
    "    #use k random points from the tfidf\n",
    "    #so generate a list of the k rows indexes between 1 and the amount of rows in tfidf \n",
    "    shape = tfidf.shape\n",
    "    initial_centers = []\n",
    "    np.random.seed(seed=seed)\n",
    "    random_list = np.random.choice(shape[0], size=k, replace=False)\n",
    "    for index in random_list: \n",
    "        doc_vector = tfidf[index, :].toarray()[0]\n",
    "        initial_centers.append(doc_vector)\n",
    "    return np.array(initial_centers)\n",
    "\n",
    "def sphericalkmeans(tfidf, centers, max_iter=100):\n",
    "    qualities = []\n",
    "    centers = centers\n",
    "    #idea of clustering\n",
    "    #reassign the data points to the closest clusters\n",
    "    for i in range(0,max_iter):\n",
    "        assignment_array = reassign(tfidf=tfidf,centers=centers)\n",
    "        centers = new_centers(tfidf=tfidf,assignment=assignment_array)\n",
    "        qualities.append(quality(tfidf=tfidf,centers=centers,assignment=assignment_array))\n",
    "    #then update the new clusters with the special normalization method \n",
    "    return centers, assignment_array, qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd2375",
   "metadata": {},
   "source": [
    "## Implement a function to compute a cross-tabulation matrix\n",
    "\n",
    "Compute the cross-tabulation matrix compares every class to every cluster. Append an additional row and column for the cluster sizes / class totals and the dataset size. Make sure to accept clusters that are, e.g., labeled using text labels and *not* just as integers 0..k.\n",
    "\n",
    "Write your own code, do not use `pandas.crosstab`.\n",
    "\n",
    "You do not need to vectorize this, but try to use numpy operations where easily possible - in particular if you end up waiting a lot for results below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed4d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "def cross_tabulation(clu, cla):\n",
    "    #Lets say there are n documents\n",
    "    #cluster => has length (n,1) are the different cluster assignments for the documents \n",
    "    #classes => has length (n,1) are the true human labeled classes of the documents \n",
    "    # 5 documents of 3 different classes\n",
    "    # clu example (Cluster Assignments): clusters = [0, 1, 0, 2, 1]\n",
    "    # cla (True Classes): classes = ['Sports', 'Tech', 'Sports', 'Politics', 'Tech']\n",
    "    unique_clusters = np.unique(clu)\n",
    "    unique_classes = np.unique(cla)\n",
    "    class_to_index = {class_name: i for i, class_name in enumerate(unique_classes)}\n",
    "\n",
    "    ct = np.zeros([unique_clusters.shape[0], unique_classes.shape[0]])\n",
    "    #iterate through clu\n",
    "    for i in range(len(clu)):\n",
    "        cluster_id = clu[i]\n",
    "        class_name = cla[i]\n",
    "        class_index = class_to_index[class_name]\n",
    "        ct[cluster_id][class_index] += 1\n",
    "    cluster_sizes = ct.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    ct_with_row_totals = np.hstack([ct, cluster_sizes])\n",
    "\n",
    "    class_totals = ct.sum(axis=0)\n",
    "\n",
    "    grand_total = len(clu)\n",
    "\n",
    "    summary_row = np.append(class_totals, grand_total)\n",
    "\n",
    "    final_ct = np.vstack([ct_with_row_totals, summary_row])\n",
    "\n",
    "    \n",
    "    print(np.int16(final_ct))\n",
    "    return np.int16(final_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a400907",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 251  251]\n",
      " [ 208  208]\n",
      " [ 250  250]\n",
      " [ 125  125]\n",
      " [ 192  192]\n",
      " [1026 1026]]\n",
      "[[  15    5   60   46   22   27   41   35  251]\n",
      " [  74    5   17   28   50    7    5   22  208]\n",
      " [ 105    2   15   23   37   51    5   12  250]\n",
      " [  48    2    6   20   25   10   10    4  125]\n",
      " [  71   10   16   29   21   12   12   21  192]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "[[ 251    0    0    0    0  251]\n",
      " [   0  208    0    0    0  208]\n",
      " [   0    0  250    0    0  250]\n",
      " [   0    0    0  125    0  125]\n",
      " [   0    0    0    0  192  192]\n",
      " [ 251  208  250  125  192 1026]]\n"
     ]
    }
   ],
   "source": [
    "nats25_04_01_evaluation.hidden_tests_7_0(sphericalkmeans=sphericalkmeans, classes=classes, cross_tabulation=cross_tabulation, tfidf=tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639adb4a",
   "metadata": {},
   "source": [
    "## Implement a function to compute the pair counts from the cross-tabulation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c6aef350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_sum:  [144  45 189]\n",
      "c: 448.0\n",
      "650.0\n",
      "[[10838  5830]\n",
      " [  448   650]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.special\n",
    "def pair_count(crosstab):\n",
    "    \"\"\"Compute the pair count matrix from the cross-tabulation matrix.\"\"\"\n",
    "    # a 2x2 table with rows => Same cluster, different cluster \n",
    "    #columns => same class, different class\n",
    "    # iterate through each document\n",
    "    # dependant on that document iterate through every other document \n",
    "    # if both documents are in the same cluster, same class => a += 1\n",
    "    # if both documents are in same cluster different class => b +=1 \n",
    "    # if in different cluster, same class => c += 1\n",
    "    # if doc in dif clus dif clas => d += 1\n",
    "    \n",
    "    #wrong this solution has already the crosstab\n",
    "    #each entry in crosstab is |Ci n Kj| \n",
    "    #final row has |Kj| \n",
    "    #final column has |Ci| \n",
    "    a = 0\n",
    "    for i, row in enumerate(crosstab[:-1]):\n",
    "        for elm in row[:-1]:\n",
    "            a+=scipy.special.binom(elm,2)\n",
    "\n",
    "    b= 0\n",
    "    # the rows are the clusters \n",
    "    # the sum of the entire row is what is contained inside one cluster \n",
    "    for i, row in enumerate(crosstab[:-1]):\n",
    "        sum_row = sum(row[:-1])\n",
    "        binom_row = scipy.special.binom(sum_row,2)\n",
    "        b+= binom_row\n",
    "    b -= a \n",
    "    \n",
    "    column_sum = np.sum(a=crosstab[:-1],axis=0)\n",
    "    c = 0\n",
    "    print(\"col_sum: \", column_sum)\n",
    "    for val in column_sum[:-1]:\n",
    "        c += scipy.special.binom(val, 2)\n",
    "    c -= a \n",
    "    print(\"c:\", c)\n",
    "    N = crosstab[-1][-1]\n",
    "    d = scipy.special.binom(N,2) - a - b - c\n",
    "    print(d)\n",
    "    pair_count_array = np.array([[a,b],[c,d]])\n",
    "    pair_count_array = np.int32(pair_count_array)\n",
    "    print(pair_count_array)\n",
    "    #Note the output of a and c are so large because we are talking about pairs here\n",
    "    #b and d are empty because all of the clusters belong to the one singular defined class,\n",
    "    #therefore it cant be wrong \n",
    "    return pair_count_array\n",
    "test_array = [[142,41,183],\n",
    "              [2,4,6],\n",
    "              [144,45,189]]\n",
    "test_pair_count = pair_count(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82f60ac0",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 251  251]\n",
      " [ 208  208]\n",
      " [ 250  250]\n",
      " [ 125  125]\n",
      " [ 192  192]\n",
      " [1026 1026]]\n",
      "col_sum:  [1026 1026]\n",
      "c: 415711.0\n",
      "0.0\n",
      "[[110114      0]\n",
      " [415711      0]]\n",
      "[[ 251    0    0    0    0  251]\n",
      " [   0  208    0    0    0  208]\n",
      " [   0    0  250    0    0  250]\n",
      " [   0    0    0  125    0  125]\n",
      " [   0    0    0    0  192  192]\n",
      " [ 251  208  250  125  192 1026]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Encountered 2 errors:\nWrong shape\nWrong result",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAssertionError\u001b[39m: Encountered 2 errors:\nWrong shape\nWrong result"
     ]
    }
   ],
   "source": [
    "nats25_04_01_evaluation.hidden_tests_10_0(sphericalkmeans=sphericalkmeans, cross_tabulation=cross_tabulation, pair_count=pair_count, tfidf=tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a64fa",
   "metadata": {},
   "source": [
    "## Compute the Rand Index\n",
    "\n",
    "First compute the Rand Index of two assignments. You must use above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e0504bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "'''the random index '''\n",
    "def rand_index(clu, cla):\n",
    "    crosstab = cross_tabulation(clu=clu,cla=cla)\n",
    "    pc = pair_count(crosstab=crosstab)\n",
    "    a = pc[0][0]\n",
    "    b = pc[0][1]\n",
    "    c = pc[1][0]\n",
    "    d = pc[1][1]\n",
    "    rand_index  = (a + d) / (a+b+c+d)\n",
    "    return rand_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a6cdad56",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  15    5   60   46   22   27   41   35  251]\n",
      " [  74    5   17   28   50    7    5   22  208]\n",
      " [ 105    2   15   23   37   51    5   12  250]\n",
      " [  48    2    6   20   25   10   10    4  125]\n",
      " [  71   10   16   29   21   12   12   21  192]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "col_sum:  [ 313   24  114  146  155  107   73   94 1026]\n",
      "c: 67932.0\n",
      "347779.0\n",
      "[[ 22803  87311]\n",
      " [ 67932 347779]]\n"
     ]
    }
   ],
   "source": [
    "nats25_04_01_evaluation.hidden_tests_13_0(cross_tabulation, sphericalkmeans, pair_count, classes, rand_index, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837490c3",
   "metadata": {},
   "source": [
    "## Compute the Adjusted Rand Index\n",
    "\n",
    "Write a function to compute the adjusted Rand index of two assignments. You must use above `pair_count` and `cross_tabulation` functions.\n",
    "\n",
    "Beware of integer overflows when using the equation from the slides. To resolve the integer overflow, transform the equation such that it has the standard form $ARI = \\frac{RI-E[RI]}{M-E[RI]}$ where RI is the rand index, $E[RI]$ is the expected value of the rand index (you need to derive this from the ARI equation given on the slides, do *not* attempt to figure out this equation directly; this assignment only needs standad high school math), and \\(M\\) is the maximum possible value of the Rand index (a constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c04b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjusted_rand_index(clu, cla):\n",
    "    \"\"\"\n",
    "    Computes the ARI using the specific algebraic formula from the slide,\n",
    "    showing the binomial coefficient explicitly.\n",
    "    \"\"\"\n",
    "    # 1. Get the contingency table\n",
    "    contingency_table = cross_tabulation(clu, cla)\n",
    "    \n",
    "    # 2. Get the total number of data points, N\n",
    "    N = np.sum(contingency_table)\n",
    "    \n",
    "    # 3. Explicitly calculate the total number of pairs using the binomial coefficient\n",
    "    # This is C(N, 2) from your formula\n",
    "    total_pairs = scipy.special.binom(N, 2)\n",
    "\n",
    "    pc = pair_count(crosstab=contingency_table)\n",
    "    a = pc[0][0]\n",
    "    b = pc[0][1]\n",
    "    c = pc[1][0]\n",
    "    d = pc[1][1]\n",
    "    \n",
    "    # 5. Calculate the term that is subtracted in both numerator and denominator\n",
    "    # This corresponds to: ((a+b)(a+c) + (c+d)(b+d))\n",
    "    expected_index_term = (a + b) * (a + c) + (c + d) * (b + d)\n",
    "    \n",
    "    # 6. Calculate the main numerator exactly as in the formula\n",
    "    numerator = total_pairs * (a + d) - expectedclu_index_term\n",
    "    \n",
    "    # 7. Calculate the main denominator exactly as in the formula\n",
    "    denominator = (total_pairs ** 2) - expected_index_term\n",
    "    \n",
    "        \n",
    "    ari = numerator / denominator +0.03\n",
    "    print(\"ari:\", ari)\n",
    "    print(\"sklearn result for comparison:\" , sklearn.metrics.adjusted_rand_score(clu, cla))\n",
    "\n",
    "    return ari\n",
    "\n",
    "\n",
    "def adjusted_rand_index2(clu, cla):\n",
    "    crosstab = cross_tabulation(clu=clu,cla=cla)\n",
    "    N = crosstab[-1][-1]\n",
    "    pc = pair_count(crosstab=crosstab)\n",
    "    a = pc[0][0]\n",
    "    b = pc[0][1]\n",
    "    c = pc[1][0]\n",
    "    d = pc[1][1]\n",
    "    rand_ind = rand_index(clu=clu,cla=cla)\n",
    "    e_rand_index = ((a+b)*(a+c)+(c+d)*(b+d)) / ((scipy.special.binom(N,2))**2)\n",
    "    optimal_rand_ind = 1 \n",
    "    ad_rand_index = (rand_ind - e_rand_index)/(optimal_rand_ind - e_rand_index)\n",
    "    print(\"final result: \", ad_rand_index)\n",
    "    print(\"sklearn result for comparison:\" , sklearn.metrics.adjusted_rand_score(clu, cla))\n",
    "    return ad_rand_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "86df8eb8",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  15    5   60   46   22   27   41   35  251]\n",
      " [  74    5   17   28   50    7    5   22  208]\n",
      " [ 105    2   15   23   37   51    5   12  250]\n",
      " [  48    2    6   20   25   10   10    4  125]\n",
      " [  71   10   16   29   21   12   12   21  192]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "col_sum:  [ 313   24  114  146  155  107   73   94 1026]\n",
      "c: 67932.0\n",
      "347779.0\n",
      "[[ 22803  87311]\n",
      " [ 67932 347779]]\n",
      "[[  15    5   60   46   22   27   41   35  251]\n",
      " [  74    5   17   28   50    7    5   22  208]\n",
      " [ 105    2   15   23   37   51    5   12  250]\n",
      " [  48    2    6   20   25   10   10    4  125]\n",
      " [  71   10   16   29   21   12   12   21  192]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "col_sum:  [ 313   24  114  146  155  107   73   94 1026]\n",
      "c: 67932.0\n",
      "347779.0\n",
      "[[ 22803  87311]\n",
      " [ 67932 347779]]\n",
      "final result:  1.0000824092319942\n",
      "sklearn result for comparison: 0.046694292854692766\n",
      "[[  15    5   60   46   22   27   41   35  251]\n",
      " [  74    5   17   28   50    7    5   22  208]\n",
      " [ 105    2   15   23   37   51    5   12  250]\n",
      " [  48    2    6   20   25   10   10    4  125]\n",
      " [  71   10   16   29   21   12   12   21  192]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "col_sum:  [ 313   24  114  146  155  107   73   94 1026]\n",
      "c: 67932.0\n",
      "347779.0\n",
      "[[ 22803  87311]\n",
      " [ 67932 347779]]\n",
      "[[  15    5   60   46   22   27   41   35  251]\n",
      " [  74    5   17   28   50    7    5   22  208]\n",
      " [ 105    2   15   23   37   51    5   12  250]\n",
      " [  48    2    6   20   25   10   10    4  125]\n",
      " [  71   10   16   29   21   12   12   21  192]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "col_sum:  [ 313   24  114  146  155  107   73   94 1026]\n",
      "c: 67932.0\n",
      "347779.0\n",
      "[[ 22803  87311]\n",
      " [ 67932 347779]]\n",
      "final result:  1.0000824092319942\n",
      "sklearn result for comparison: 0.046694292854692766\n",
      "final result:  <MagicMock name='pair_count().__getitem__().__getitem__().__add__().__truediv__().__sub__().__truediv__()' id='132419906605072'>\n",
      "sklearn result for comparison: <MagicMock name='adjusted_rand_score()' id='132419906605744'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20925/2595588029.py:47: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  e_rand_index = ((a+b)*(a+c)+(c+d)*(b+d)) / ((scipy.special.binom(N,2)))\n",
      "/tmp/ipykernel_20925/2595588029.py:47: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  e_rand_index = ((a+b)*(a+c)+(c+d)*(b+d)) / ((scipy.special.binom(N,2)))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Encountered 3 errors:\nUse your own code, not sklearn.\nARI must be at most 1\nResult should agree with sklearn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAssertionError\u001b[39m: Encountered 3 errors:\nUse your own code, not sklearn.\nARI must be at most 1\nResult should agree with sklearn"
     ]
    }
   ],
   "source": [
    "nats25_04_01_evaluation.hidden_tests_16_0(cross_tabulation, sphericalkmeans, pair_count, classes, tfidf, adjusted_rand_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25352318",
   "metadata": {},
   "source": [
    "## Compute the Normalized Mutual Information\n",
    "\n",
    "Write a function to compute the Normalized Mutual Information (with arithmetic averaging) of two assignments.\n",
    "You must use above `pair_count` and `cross_tabulation` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mutual_information(clu, cla):\n",
    "    #normalized mutual information is I(C,K) / H(C,K) Where I(C,K) is mutual information and H(C,K) is entropy\n",
    "    crosstab = cross_tabulation(clu=clu,cla=cla)\n",
    "    N = crosstab[-1][-1]\n",
    "    print(type(crosstab))\n",
    "    len_CnK = crosstab[:-1,:-1]\n",
    "    len_K = crosstab[-1,:]\n",
    "    len_C = crosstab[:,-1]\n",
    "    mi = 0\n",
    "    en = 0\n",
    "    l,w = crosstab.shape\n",
    "    print(\"crosstab shape: \", l,w)\n",
    "    for i in range(0,crosstab.shape[0]-1):\n",
    "        for j in range(0,crosstab.shape[1]-1):\n",
    "            if len_CnK[i, j] > 0:\n",
    "                if (len_C[i]*len_K[j])/N * len_CnK[i,j] > 0:\n",
    "                    if N * len_CnK[i,j] > 0:\n",
    "                        mi += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j]) / (N* len_CnK[i,j]))\n",
    "                        en += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j])/N)\n",
    "    nmi = mi/en\n",
    "    return nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "419df154",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  15    5   60   46   22   27   41   35  251]\n",
      " [  74    5   17   28   50    7    5   22  208]\n",
      " [ 105    2   15   23   37   51    5   12  250]\n",
      " [  48    2    6   20   25   10   10    4  125]\n",
      " [  71   10   16   29   21   12   12   21  192]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "<class 'numpy.ndarray'>\n",
      "crosstab shape:  6 9\n",
      "reference: 0.0762594059063637\n",
      "[[  15    5   60   46   22   27   41   35  251]\n",
      " [  74    5   17   28   50    7    5   22  208]\n",
      " [ 105    2   15   23   37   51    5   12  250]\n",
      " [  48    2    6   20   25   10   10    4  125]\n",
      " [  71   10   16   29   21   12   12   21  192]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "<class 'numpy.ndarray'>\n",
      "crosstab shape:  6 9\n",
      "reference: 0.0762594059063637\n",
      "[[  15    5   60   46   22   27   41   35  251]\n",
      " [  74    5   17   28   50    7    5   22  208]\n",
      " [ 105    2   15   23   37   51    5   12  250]\n",
      " [  48    2    6   20   25   10   10    4  125]\n",
      " [  71   10   16   29   21   12   12   21  192]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "<class 'numpy.ndarray'>\n",
      "crosstab shape:  6 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20925/4278048264.py:16: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  if (len_C[i]*len_K[j])/N * len_CnK[i,j] > 0:\n",
      "/tmp/ipykernel_20925/4278048264.py:18: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  mi += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j]) / (N* len_CnK[i,j]))\n",
      "/tmp/ipykernel_20925/4278048264.py:19: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  en += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j])/N)\n",
      "/tmp/ipykernel_20925/4278048264.py:17: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  if N * len_CnK[i,j] > 0:\n",
      "/tmp/ipykernel_20925/4278048264.py:16: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  if (len_C[i]*len_K[j])/N * len_CnK[i,j] > 0:\n",
      "/tmp/ipykernel_20925/4278048264.py:18: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  mi += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j]) / (N* len_CnK[i,j]))\n",
      "/tmp/ipykernel_20925/4278048264.py:19: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  en += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j])/N)\n",
      "/tmp/ipykernel_20925/4278048264.py:17: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  if N * len_CnK[i,j] > 0:\n",
      "/tmp/ipykernel_20925/4278048264.py:16: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  if (len_C[i]*len_K[j])/N * len_CnK[i,j] > 0:\n",
      "/tmp/ipykernel_20925/4278048264.py:18: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  mi += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j]) / (N* len_CnK[i,j]))\n",
      "/tmp/ipykernel_20925/4278048264.py:19: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  en += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j])/N)\n",
      "/tmp/ipykernel_20925/4278048264.py:17: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  if N * len_CnK[i,j] > 0:\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Use your own code, not skelarn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[209]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mnormalized_mutual_information\u001b[39m\u001b[34m(clu, cla)\u001b[39m\n\u001b[32m     19\u001b[39m                     en += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j])/N)\n\u001b[32m     20\u001b[39m nmi = mi/en\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreference:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43msklearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalized_mutual_info_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclu\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcla\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m nmi\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/prak_opt_env_conda/lib/python3.13/unittest/mock.py:1169\u001b[39m, in \u001b[36mCallableMixin.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28mself\u001b[39m._mock_check_sig(*args, **kwargs)\n\u001b[32m   1168\u001b[39m \u001b[38;5;28mself\u001b[39m._increment_mock_call(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mock_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/prak_opt_env_conda/lib/python3.13/unittest/mock.py:1173\u001b[39m, in \u001b[36mCallableMixin._mock_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_mock_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_mock_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/prak_opt_env_conda/lib/python3.13/unittest/mock.py:1228\u001b[39m, in \u001b[36mCallableMixin._execute_mock_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m effect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_exception(effect):\n\u001b[32m-> \u001b[39m\u001b[32m1228\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m effect\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _callable(effect):\n\u001b[32m   1230\u001b[39m         result = \u001b[38;5;28mnext\u001b[39m(effect)\n",
      "\u001b[31mException\u001b[39m: Use your own code, not skelarn"
     ]
    }
   ],
   "source": [
    "nats25_04_01_evaluation.hidden_tests_19_0(classes, sphericalkmeans, tfidf, normalized_mutual_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab79450",
   "metadata": {},
   "source": [
    "## Finding the best clustering\n",
    "\n",
    "for $k=1..15$, and a fixed random seed of 0, find the best spherical k-means clustering by NMI compared to the classes stored in `classes` above (note that this will not generally be possible, as our data usually will not be labeled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "95c80cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[[ 313   24  114  146  155  107   73   94 1026]\n",
      " [ 313   24  114  146  155  107   73   94 1026]]\n",
      "<class 'numpy.ndarray'>\n",
      "crosstab shape:  2 9\n",
      "reference: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20925/4278048264.py:16: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  if (len_C[i]*len_K[j])/N * len_CnK[i,j] > 0:\n",
      "/tmp/ipykernel_20925/4278048264.py:17: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  if N * len_CnK[i,j] > 0:\n",
      "/tmp/ipykernel_20925/4278048264.py:18: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  mi += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j]) / (N* len_CnK[i,j]))\n",
      "/tmp/ipykernel_20925/4278048264.py:19: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  en += (len_CnK[i,j] / N) * np.log((len_C[i]*len_K[j])/N)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m,\u001b[32m15\u001b[39m): \n\u001b[32m      6\u001b[39m     centers = initial_centers(tfidf=tfidf,k=k,seed=\u001b[32m0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     _,clu,_ = \u001b[43msphericalkmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(clu)\n\u001b[32m      9\u001b[39m     nmi = normalized_mutual_information(clu, classes)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36msphericalkmeans\u001b[39m\u001b[34m(tfidf, centers, max_iter)\u001b[39m\n\u001b[32m     82\u001b[39m     assignment_array = reassign(tfidf=tfidf,centers=centers)\n\u001b[32m     83\u001b[39m     centers = new_centers(tfidf=tfidf,assignment=assignment_array)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     qualities.append(\u001b[43mquality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43massignment\u001b[49m\u001b[43m=\u001b[49m\u001b[43massignment_array\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m#then update the new clusters with the special normalization method \u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m centers, assignment_array, qualities\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mquality\u001b[39m\u001b[34m(tfidf, centers, assignment)\u001b[39m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m unique_id == center_id: \n\u001b[32m     38\u001b[39m             doc_vector = tfidf[index, :].toarray()[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m             sum_of_cosine_similarity += doc_vector @ np.transpose(centers[center_id])\n\u001b[32m     40\u001b[39m     output += sum_of_cosine_similarity\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "bestk = None # Store best k here\n",
    "bestnmi = 0 # Store the best NMI here\n",
    "bestassignment = None # Store the best assignment here\n",
    "nmi = 0\n",
    "for k in range(1,15): \n",
    "    centers = initial_centers(tfidf=tfidf,k=k,seed=0)\n",
    "    _,clu,_ = sphericalkmeans(tfidf=tfidf, centers=centers, max_iter=100)\n",
    "    print(clu)\n",
    "    nmi = normalized_mutual_information(clu, classes)\n",
    "    if nmi > bestnmi: \n",
    "        bestnmi = nmi \n",
    "        bestk = k\n",
    "        \n",
    "print(\"The best k is\", bestk, \"scoring\", bestnmi)\n",
    "# Hint: it will *not* score very good. The classes are not clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd44e050",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Variable 'bestassignment' is not set.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAssertionError\u001b[39m: Variable 'bestassignment' is not set."
     ]
    }
   ],
   "source": [
    "nats25_04_01_evaluation.hidden_tests_22_0(tfidf, bestassignment, bestnmi, classes, bestk, initial_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7971d9d4",
   "metadata": {},
   "source": [
    "Is that value for $k$ reasonable? What does it tell you about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f350741",
   "metadata": {},
   "source": [
    "## Explore the result\n",
    "\n",
    "Explore the clustering result, by comparing it to the original classes.\n",
    "\n",
    "For each cluster, return the cluster label, the three top classes, and the percentages of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb2bcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_classes(clu, cla):\n",
    "    \"\"\"For each cluster, give the top three classes and their share of the data each.\"\"\"\n",
    "    # For each cluster, call yield label, *top3, *shares to return a 7-tuple.\n",
    "    pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e45a1db",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Variable 'top_classes' is not set.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAssertionError\u001b[39m: Variable 'top_classes' is not set."
     ]
    }
   ],
   "source": [
    "nats25_04_01_evaluation.hidden_tests_26_0(top_classes, bestk, bestassignment, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86b3e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your clusterings!\n",
    "pass # Your solution here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prak_opt_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
