{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scikit-learn gensim\n",
    "%pip install --no-cache-dir --force-reinstall https://dm.cs.tu-dortmund.de/nats/nats25_07_01_word2vec-0.1-py3-none-any.whl\n",
    "import nats25_07_01_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1073dbe",
   "metadata": {},
   "source": [
    "# Explore pre-trained word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778903a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def md5_checksum(fname):\n",
    "  'MD5 checksum function to validate that the large files were downloaded correctly'\n",
    "  hash_md5 = hashlib.md5()\n",
    "  with open(fname, \"rb\") as f:\n",
    "    for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "      hash_md5.update(chunk)\n",
    "  return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d152244",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the input data - do not modify\n",
    "import json, gzip, urllib\n",
    "import numpy as np\n",
    "file_path, _ = urllib.request.urlretrieve(\"https://dm.cs.tu-dortmund.de/nats/data/minecraft-articles.json.gz\")\n",
    "raw = json.load(gzip.open(file_path, \"rt\", encoding=\"utf-8\"))\n",
    "titles, texts, classes = [x[\"title\"] for x in raw], [x[\"text\"] for x in raw], [x[\"heuristic\"] for x in raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340155fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the pretrained word2vec model from Google\n",
    "# These files are large (approx. 200MB in total)! It will take some time.\n",
    "all_paths = []\n",
    "for url, checksum in [\n",
    "    [\"https://dm.cs.tu-dortmund.de/nats/data/w2v-google-news.wordvectors.got_subset.words.csv.gz\", \"de96f3aa4dee24c6905b79e9a5c6eeb8\"],\n",
    "    [\"https://dm.cs.tu-dortmund.de/nats/data/w2v-google-news.wordvectors.got_subset.vectors00.npy\", \"045af08301542a0ddd560f85d07a198f\"],\n",
    "    [\"https://dm.cs.tu-dortmund.de/nats/data/w2v-google-news.wordvectors.got_subset.vectors01.npy\", \"51c147e4bdee65c95cb47d96dd818a7b\"],\n",
    "    [\"https://dm.cs.tu-dortmund.de/nats/data/w2v-google-news.wordvectors.got_subset.vectors02.npy\", \"7fcc8314539a9e7b83b7703fb97ce890\"],\n",
    "    [\"https://dm.cs.tu-dortmund.de/nats/data/w2v-google-news.wordvectors.got_subset.vectors03.npy\", \"b68f15aca97993103852c62ce5cf49a5\"],\n",
    "    [\"https://dm.cs.tu-dortmund.de/nats/data/w2v-google-news.wordvectors.got_subset.vectors04.npy\", \"8a39594c9e9aa3cf07f2aa624de16d96\"],\n",
    "    [\"https://dm.cs.tu-dortmund.de/nats/data/w2v-google-news.wordvectors.got_subset.vectors05.npy\", \"96cc1f43c6865d700f7df479b5bb30c5\"],\n",
    "]:\n",
    "    file_path, _ = urllib.request.urlretrieve(url)\n",
    "    assert md5_checksum(file_path) == checksum, f\"Corrupted file '{file_path}'. Please delete manually and restart this cell.\\nIf the error persists, try reloading this page.\"\n",
    "    all_paths.append(file_path)\n",
    "words_path, vec_chunk_paths = all_paths[0], all_paths[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the vocabulary and vector representations into a KeyedVectors Gensim model\n",
    "from gensim.models import KeyedVectors\n",
    "with gzip.open(words_path, \"rt\") as f: words = f.read().split(\"\\n\")\n",
    "vecs = np.concatenate([\n",
    "    np.load(file_path)\n",
    "    for file_path in vec_chunk_paths\n",
    "], axis=0)\n",
    "model = KeyedVectors(count=vecs.shape[0],vector_size=vecs.shape[1],dtype=vecs.dtype)\n",
    "model.add_vectors(words, vecs)\n",
    "model.fill_norms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a032b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 10 most similar words to \"Stone\"\n",
    "most_stone = None # words only\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75d92a",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_07_01_word2vec.hidden_tests_6_0(most_stone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0c460",
   "metadata": {},
   "source": [
    "## Verify the classic king-queen example\n",
    "\n",
    "Verify that \"King - Man + Woman = Queen\", using the built-in function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1daca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_kmw = None # 10 nearest words to \"king-man+woman\" using the gensim API\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031adad2",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_07_01_word2vec.hidden_tests_9_0(most_kmw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d646a302",
   "metadata": {},
   "source": [
    "## Try using Euclidean geometry\n",
    "\n",
    "Get the vectors for king, man, queen, and woman.\n",
    "\n",
    "Compute king-man+woman, and compute the distances to each of above four words. What word is closest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "king, man, queen, woman = None, None, None, None # get the word vectors\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46251366",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = king - man + woman\n",
    "for word, vec in [(\"king\", king), (\"man\", man), (\"woman\", woman), (\"queen\", queen)]:\n",
    "    score = np.sqrt(((target - vec)**2).sum())\n",
    "    print(\"distance(king - man + woman, %s) = %.5f\" % (word, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fc6d5",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_07_01_word2vec.hidden_tests_13_0(queen, man, model, king, woman)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9768d1a",
   "metadata": {},
   "source": [
    "## Document representations\n",
    "\n",
    "Represent each document as the average word2vec vector of all words present in the model. Do not normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = np.zeros((len(titles), 300))\n",
    "from gensim.utils import tokenize\n",
    "for i, (title, text) in enumerate(zip(titles, texts)):\n",
    "    tokens = tokenize(title + \"\\n\" + text)\n",
    "    pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75a81c",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_07_01_word2vec.hidden_tests_16_0(titles, document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce04415",
   "metadata": {},
   "source": [
    "## Find the document with the shortest vector\n",
    "\n",
    "Note: this likely will be one of the longer documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248579e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest = None # Document number of the document with the shortest vector\n",
    "pass # Your solution here\n",
    "print(titles[shortest], len(texts[shortest]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53174ac3",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_07_01_word2vec.hidden_tests_19_0(shortest, titles, document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee23e8",
   "metadata": {},
   "source": [
    "## Find the two most similar documents\n",
    "\n",
    "Compute a similarity matrix, and find the largest pair of articles.\n",
    "\n",
    "Do *not* use nested for loops, this will timeout (use the \"Validate\" option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a similarity matrix\n",
    "similarity_matrix = None\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a96a4cb",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_07_01_word2vec.hidden_tests_22_0(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e47c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar = None # Pair of two different documents\n",
    "pass # Your solution here\n",
    "print(titles[most_similar[0]], \" and \", titles[most_similar[1]])\n",
    "print(len(texts[most_similar[0]]), \" and \", len(texts[most_similar[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16703f72",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_07_01_word2vec.hidden_tests_24_0(similarity_matrix, most_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6aabb9",
   "metadata": {},
   "source": [
    "## Find the two most similar longer documents\n",
    "\n",
    "Now only consider documents that have at least 10000 characters in the body!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc91047",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar = None # Pair of two different documents\n",
    "pass # Your solution here\n",
    "print(titles[most_similar[0]], \" and \", titles[most_similar[1]])\n",
    "print(\"Lengths:\", len(texts[most_similar[0]]), \" and \", len(texts[most_similar[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62687f3d",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_07_01_word2vec.hidden_tests_27_0(similarity_matrix, most_similar, titles, texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6558883",
   "metadata": {},
   "source": [
    "## Run k-means and spherical k-means\n",
    "\n",
    "Cluster the document vectors (*not* the similarity matrix) with spherical k-means.\n",
    "\n",
    "Use k=10, and a fixed random seed of 42.\n",
    "\n",
    "Recall the assumptions of our spherical k-means implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04775da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcent = None # Compute the k-means cluster centers\n",
    "kassi = None # Compute the k-means cluster assignment\n",
    "from sklearn.cluster import KMeans\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00022ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimalistic implementation for spherical k-means, so we use the same version in this assignment\n",
    "# This is NOT meant as an example of good code, but to be short.\n",
    "def initial_centers(X, k, seed):\n",
    "    return X[np.random.default_rng(seed=seed).choice(X.shape[0], k, replace=False)]\n",
    "\n",
    "def sphericalkmeans(X, centers, max_iter=100):\n",
    "    assert abs((X**2).sum()-len(X)) < 1e-7, \"Improper input for spherical k-means!\"\n",
    "    last_assignment = None\n",
    "    for iter in range(max_iter):\n",
    "        assignment = np.asarray((X @ centers.T).argmax(axis=1)).squeeze()\n",
    "        if last_assignment is not None and all(assignment == last_assignment): break\n",
    "        last_assignment, centers = assignment, np.zeros(centers.shape)\n",
    "        for i in range(centers.shape[0]):\n",
    "            c_assignment = assignment == i\n",
    "            if np.sum(c_assignment) == 0:\n",
    "                # If no points were assigned, do not move the center\n",
    "                continue\n",
    "            c = np.sum(X[c_assignment,:], axis=0)\n",
    "            centers[i] = c / np.linalg.norm(c)\n",
    "    return centers, assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scent = None # Compute the spherical k-means cluster centers\n",
    "sassi = None # Compute the spherical k-means cluster assignment\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dfc2ae",
   "metadata": {},
   "source": [
    "## Explore your result\n",
    "\n",
    "Explore the result: write a function to determine the most important words for each factor, and the most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8827c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_central(tfidf, centers, assignment, i, k=5):\n",
    "    \"\"\"Find the most central documents of cluster i\"\"\"\n",
    "    pass # Your solution here\n",
    "\n",
    "def explain(tfidf, titles, classes, centers, assignment):\n",
    "    \"\"\"Explain the clusters: print\n",
    "    (1) relative size of each cluster\n",
    "    (2) three most frequent classes of each cluster\n",
    "    (3) five most central documents of each cluster\n",
    "    (4) ARI of the entire clustering\"\"\"\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    from collections import Counter\n",
    "    pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ae417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Regular k-means clustering:\")\n",
    "explain(document_vectors, titles, classes, kcent, kassi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebfb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: in case of poor performance, revisit your code above!\n",
    "print(\"Spherical k-means clustering:\")\n",
    "explain(document_vectors, titles, classes, scent, sassi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa320bd",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_07_01_word2vec.hidden_tests_36_0(document_vectors, scent, sassi, titles, classes, explain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
