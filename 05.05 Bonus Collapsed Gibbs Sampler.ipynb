{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc827f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scikit-learn\n",
    "%pip install --no-cache-dir --force-reinstall https://dm.cs.tu-dortmund.de/nats/nats25_05_05_bonus_collapsed_gibbs_sampler-0.1-py3-none-any.whl\n",
    "import nats25_05_05_bonus_collapsed_gibbs_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa8590",
   "metadata": {},
   "source": [
    "# Implement LDA with Gibbs Sampling\n",
    "\n",
    "In this *bonus* assignment, your task is to implement LDA yourself using Gibbs sampling, without using libraries such as Gensim or sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c290b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the input data - do not modify\n",
    "import json, gzip, urllib, numpy as np\n",
    "file_path, _ = urllib.request.urlretrieve(\"https://dm.cs.tu-dortmund.de/nats/data/minecraft-articles.json.gz\")\n",
    "raw = json.load(gzip.open(file_path, \"rt\", encoding=\"utf-8\"))\n",
    "titles, texts, classes = [x[\"title\"] for x in raw], [x[\"text\"] for x in raw], [x[\"heuristic\"] for x in raw]\n",
    "# To speed up processing times, we will make use of numba.\n",
    "# Yet, there is no numba port for WebAssembly yet, so if\n",
    "# you are running this on Pyodide/JupyterLite, we need to\n",
    "# replace the jit decorator with a no-op stub.\n",
    "if not \"pyodide\" in globals():\n",
    "    %pip install numpy==2.1.3 # currently, numba does not support 2.2\n",
    "    %pip install numba\n",
    "    from numba import jit\n",
    "else:\n",
    "    # Use no-op stub.\n",
    "    def jit(*args, **kwargs): return lambda function: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1326ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorize the text - do not modify\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvect = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "counts = cvect.fit_transform(texts)\n",
    "vocabulary = cvect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how the non-zero values are stored / accessible - read the docs!\n",
    "print(\".shape\", counts.shape)\n",
    "print(\".data shape:\", counts.data.shape)\n",
    "print(\".indices shape:\", counts.indices.shape)\n",
    "print(\".indptr shape:\", counts.indptr.shape)\n",
    "# work on these data structures directly, avoid any operation that copies data such as .nonzero()!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7be42",
   "metadata": {},
   "source": [
    "## Initial random labeling\n",
    "\n",
    "LDA begins with a random labeling of the tokens. For simplicity, we ignore that words can occur multiple times in a document, and always label them the same way. We want to store our labels in a dense data structure for efficiency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c19bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_labeling(counts, num_topics, rng):\n",
    "    \"\"\"Generate a uniform random labeling of the desired shape\"\"\"\n",
    "    pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387894c",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_05_05_bonus_collapsed_gibbs_sampler.hidden_tests_6_0(counts, initial_labeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452d766",
   "metadata": {},
   "source": [
    "## Initial statistics\n",
    "\n",
    "Compute the initial count statistics of the labeling necessary for the Collapsed Gibbs Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e65373",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True) # compile, otherwise this may be way too slow\n",
    "def initial_statistics(indices, indptr, labels, num_docs, num_words, num_topics):\n",
    "    pass # Your solution here\n",
    "    return doc_topic, topic_word, topic # counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af691c",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_05_05_bonus_collapsed_gibbs_sampler.hidden_tests_9_0(counts, initial_labeling, initial_statistics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c553be",
   "metadata": {},
   "source": [
    "## Collapsed Gibbs Sampler\n",
    "\n",
    "Implement one step (one pass over the data set) of the collapsed Gibbs sampler.\n",
    "\n",
    "We still ignore if words occur multiple times for simplicity.\n",
    "\n",
    "You may (and will need to) modify the data structures in-place, because this is a Markov process.\n",
    "Remove the label, sample a new label, add new label, repeat.\n",
    "\n",
    "Because numba does not support current numpy random generator objects, nor weighted random choice, we need to implement our own helper first. But we also need to use compilation: with pure python one pass easily takes 45 seconds as opposed to 250 ms, i.e., the compiled code runs 180x faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789bafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True) # compile, otherwise this may be way too slow\n",
    "def weighted_random(p):\n",
    "    \"\"\"Choose an index of p randomly, weighted by values in p (which must sum to 1)\"\"\"\n",
    "    pass # Your solution here\n",
    "    # Hint: add a fallback for the (rare) case that the sum is slightly less than 1:\n",
    "    return int(np.floor(np.random.rand() * p.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae288ebe",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_05_05_bonus_collapsed_gibbs_sampler.hidden_tests_12_0(weighted_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d88fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True) # compile, otherwise this may be way too slow\n",
    "def gibbs(alpha, eta, indices, indptr, labels, doc_topic, topic_word, topic):\n",
    "    num_topics = topic.shape[0]\n",
    "    pass # Your solution here\n",
    "    return labels # same data structure as above, for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b560f7",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_05_05_bonus_collapsed_gibbs_sampler.hidden_tests_14_0(counts, gibbs, initial_labeling, initial_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9b429",
   "metadata": {},
   "source": [
    "## Implement LDA with Gibbs sampling\n",
    "\n",
    "Write the outer loop of LDA. We will use a burn-in of 50 iterations, then aggregate 50 subsequent iterations to obtain the summary statistics. At 100 iterations, 250 ms above will likely be a tolerable 25 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408250e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no jit: the outer loop should be fine with just numpy + calls to the compiled functions above\n",
    "def lda(counts, k, alpha, eta, rng, burnin=50, measure=50, every=1):\n",
    "    \"\"\"Latent Dirichlet Allocation. Return the factors and document assignment\"\"\"\n",
    "    np.random.seed(rng.integers(0x7FFFFFF)) # needed for numba\n",
    "    pass # Your solution here\n",
    "    return factors, assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75134bd0",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_05_05_bonus_collapsed_gibbs_sampler.hidden_tests_17_0(counts, lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72737b",
   "metadata": {},
   "source": [
    "\n",
    "## Explore your result\n",
    "\n",
    "Explore the result: write a function to determine the most important words for each factor, and the most relevant documents.\n",
    "\n",
    "**COPY your code from the first file here** (one of the rare cases, where copying is okay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important(vocabulary, factor, k=10):\n",
    "    \"\"\"Most important words for each factor\"\"\"\n",
    "    pass # Your solution here\n",
    "\n",
    "def most_relevant(assignment, k=5):\n",
    "    \"\"\"Most relevant documents for each factor (return document indexes)\"\"\"\n",
    "    pass # Your solution here\n",
    "\n",
    "def explain(vocabulary, titles, classes, factors, assignment, weights=None):\n",
    "    \"\"\"Print an explanation for each factor.\n",
    "       If weights is None, use the relative share of the assignment weights.\n",
    "       Print the ARI when assigning each document to its maximum only.\"\"\"\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024675e",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_05_05_bonus_collapsed_gibbs_sampler.hidden_tests_20_0(counts, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64818d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your result (reduce the number of iterations prior to validating/submitting, to not get a timeout)\n",
    "rng = np.random.default_rng(0)\n",
    "%time lda_factors, lda_assignment = lda(counts, 8, 1/8, 1/8, rng, 50, 50)\n",
    "explain(vocabulary, titles, classes, lda_factors, lda_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2e1b3",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_05_05_bonus_collapsed_gibbs_sampler.hidden_tests_22_0(most_important, lda_factors, vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
