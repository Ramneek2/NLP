{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scipy plotly tqdm\n",
    "%pip install --no-cache-dir --force-reinstall https://dm.cs.tu-dortmund.de/nats/nats25_09_01_attention-0.1-py3-none-any.whl\n",
    "import nats25_09_01_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b7b95",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "from scipy.sparse import csr_array\n",
    "from copy import deepcopy\n",
    "import nbbootstrap\n",
    "await nbbootstrap.ensure_package(\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f16e4",
   "metadata": {},
   "source": [
    "## Backpropagation Again\n",
    "\n",
    "For this notebook, you will be asked to write an Attention-based model and train it. To do so, you will need most of the modules built in the last assignment. The code frames for the the required network modules underneath are the same as in the last assignment and contain valid solutions.\n",
    "\n",
    "This time, the modules must be compatible to pass multiple vectors through `forward` and `backward` at once.\n",
    "\n",
    "E.g. the `Linear` module must work with `forward` input `X` of shape $n \\times d_{in}$ and return $n \\times d_{out}$ matrices. During `backward` it will receive an $n \\times d_{out}$ `delta` and must return a `next_delta` of shape $n \\times d_{in}$. The stored gradients should have shape $d_{in} \\times d_{out}$ for the weights and $d_{out}$ for the bias. You can aggregate the $n$ individual gradients by taking their mean, which simulates $n$ individual gradient steps at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f8e7cd",
   "metadata": {},
   "source": [
    "## Abstract and architecture modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb825fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkModule(ABC):\n",
    "    @abstractmethod\n",
    "    def _forward(self, X):\n",
    "        return None\n",
    "    def forward(self, X):\n",
    "        self.last_input = deepcopy(X)\n",
    "        return self._forward(X)\n",
    "    @abstractmethod\n",
    "    def backward(self, delta): pass\n",
    "    def step_gradient(self, learning_rate): pass\n",
    "class TrainableModule(NetworkModule):\n",
    "    def __init__(self):\n",
    "        self.last_input = None\n",
    "        self.grad = None\n",
    "    def backward(self, delta):\n",
    "        if self.last_input is None:\n",
    "            raise AssertionError(\"Tried to execute backpropagation without forward feeding data.\")\n",
    "        next_delta = self._next_delta(delta)\n",
    "        self.grad = self._gradient_for_last_input(delta)\n",
    "        return next_delta\n",
    "    @abstractmethod\n",
    "    def _next_delta(self, delta):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def _gradient_for_last_input(self, delta):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def step_gradient(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb50550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleChain(NetworkModule):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "    def _forward(self, X):\n",
    "        for m in self.modules:\n",
    "            X = m.forward(X)\n",
    "        return X\n",
    "    def backward(self, delta):\n",
    "        for m in reversed(self.modules):\n",
    "            delta = m.backward(delta)\n",
    "        return delta\n",
    "    def step_gradient(self, learning_rate):\n",
    "        for m in self.modules:\n",
    "            m.step_gradient(learning_rate)\n",
    "class ModuleConcat(NetworkModule):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "    def _forward(self, X):\n",
    "        parts = [m.forward(x) for m,x in zip(self.modules,X)]\n",
    "        return np.concatenate(parts, axis=-1)\n",
    "    def backward(self, delta):\n",
    "        assert delta.shape[-1] % len(self.modules) == 0\n",
    "        delta_part_length = delta.shape[-1] // len(self.modules)\n",
    "        forwarded_delta_parts = []\n",
    "        for i_module, m in enumerate(self.modules):\n",
    "            forwarded_delta_parts.append(\n",
    "                m.backward(delta[..., delta_part_length*i_module: delta_part_length*(i_module+1)])\n",
    "            )\n",
    "        return forwarded_delta_parts\n",
    "    def step_gradient(self, learning_rate):\n",
    "        for m in self.modules:\n",
    "            m.step_gradient(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001da80e",
   "metadata": {},
   "source": [
    "## Trained modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(TrainableModule):\n",
    "    def __init__(self, n_in, n_out, use_bias=True):\n",
    "        '''\n",
    "        Creates a fully connected linear layer translating from vectors of length `n_in` to vectors of length `n_out`.\n",
    "        `use_bias` controls whether or not this layer should use a bias (`f(x) = x^T W+b`) or not (`f(x) = x^T W`).\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.weights = np.random.sample((n_in, n_out))\n",
    "        self.weights /= np.sum(self.weights,axis=0,keepdims=True)\n",
    "        self.use_bias = use_bias\n",
    "        self.bias = np.random.sample(n_out)\n",
    "        self.bias /= np.sum(self.bias)\n",
    "    def _forward(self, X):\n",
    "        if self.use_bias:\n",
    "            return X.dot(self.weights) + self.bias\n",
    "        return X.dot(self.weights)\n",
    "    def _next_delta(self, delta):\n",
    "        return delta.dot(self.weights.T)\n",
    "    def _gradient_for_last_input(self, delta):\n",
    "        if type(self.last_input) == csr_array:\n",
    "            if self.last_input.shape[0] == 1:\n",
    "                self.last_input = self.last_input.toarray().flatten()\n",
    "            else:\n",
    "                self.last_input = self.last_input.toarray()\n",
    "        if len(delta.shape) == 1:\n",
    "            weight_grad = np.outer(self.last_input, delta)\n",
    "            bias_grad = delta.copy()\n",
    "        else:\n",
    "            weight_grad = np.mean([np.outer(i,o) for i,o in zip(self.last_input, delta)], axis=0)\n",
    "            bias_grad = np.mean(delta,axis=0)\n",
    "        return (weight_grad, bias_grad)\n",
    "    def step_gradient(self, learning_rate):\n",
    "        self.weights -= learning_rate * self.grad[0]\n",
    "        if self.use_bias: self.bias -= learning_rate * self.grad[1].flatten()\n",
    "        self.grad = None\n",
    "class LinkedLinear(Linear):\n",
    "    def __init__(self, other_linear, use_bias=True):\n",
    "        # You probably want to start by invoking the `Linear` constructor.\n",
    "        super().__init__(*other_linear.weights.shape, use_bias=use_bias)\n",
    "        self.weights = other_linear.weights\n",
    "class LinkedTransposedLinear(Linear):\n",
    "    def __init__(self, other_linear, use_bias=True):\n",
    "        # You probably want to start by invoking the `Linear` constructor.\n",
    "        super().__init__(*other_linear.weights.T.shape, use_bias=use_bias)\n",
    "        self.weights = other_linear.weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c11bf",
   "metadata": {},
   "source": [
    "## Activation function modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084fada",
   "metadata": {},
   "source": [
    "**Take care:** The `Softmax` class from the last assignment is renamed to `SparseSoftmax`! The *additional* `DenseSoftmax` is to be used for dense vector representations, i.e. where `delta` is a dense vector. In the Attention, we will need a dense implementation of `Softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91330f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(NetworkModule):\n",
    "    def _forward(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    def backward(self, delta):\n",
    "        return (self.last_input > 0) * delta\n",
    "class Sigmoid(NetworkModule):\n",
    "    def _forward(self, X):\n",
    "        return 1/(1+np.exp(-X))\n",
    "    def backward(self, delta):\n",
    "        v = self.forward(self.last_input)\n",
    "        return v * (1-v) * delta\n",
    "class SparseSoftmax(NetworkModule):\n",
    "    def _forward(self, X):\n",
    "        X = X-np.max(X, axis=-1, keepdims=True)\n",
    "        X_exp = np.exp(X)\n",
    "        return X_exp / np.sum(X_exp, axis=-1, keepdims=True)\n",
    "    def backward(self, delta):\n",
    "        assert type(delta) == csr_array\n",
    "        # Make last input matrix-shaped if it was a vector\n",
    "        if len(self.last_input.shape) == 1:\n",
    "            in_softmax = self._forward(self.last_input)\n",
    "            for i, j, delta_j in zip(delta.indptr, delta.indices, delta.data):\n",
    "                softmax_deriv_j = -in_softmax * in_softmax[j]\n",
    "                softmax_deriv_j[j] += in_softmax[j]\n",
    "                next_delta = softmax_deriv_j * delta_j\n",
    "                return next_delta\n",
    "        next_delta = np.zeros(self.last_input.shape)\n",
    "        for i, j, delta_j in zip(delta.indptr, delta.indices, delta.data):\n",
    "            in_softmax = self._forward(self.last_input[i])\n",
    "            softmax_deriv_j = -in_softmax * in_softmax[j]\n",
    "            softmax_deriv_j[j] += in_softmax[j]\n",
    "            next_delta[i] = softmax_deriv_j * delta_j\n",
    "        return next_delta\n",
    "class DenseSoftmax(NetworkModule):\n",
    "    def _forward(self, X):\n",
    "        X = X-np.max(X, axis=-1, keepdims=True)\n",
    "        X_exp = np.exp(X)\n",
    "        return X_exp / np.sum(X_exp, axis=-1, keepdims=True)\n",
    "    def backward(self, delta):\n",
    "        result = np.zeros(self.last_input.shape)\n",
    "        for i in range(len(result)):\n",
    "            s = self._forward(self.last_input[i])\n",
    "            dense_jacobian = np.diag(s) - np.outer(s,s)\n",
    "            result[i] = dense_jacobian.dot(delta[i])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b061e",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, prediction, target): pass\n",
    "    @abstractmethod\n",
    "    def backward(self, prediction, target): pass\n",
    "class L2Loss(Loss):\n",
    "    def forward(self, prediction, target):\n",
    "        return 1/2 * np.sum(np.square(prediction-target), axis=-1)\n",
    "    def backward(self, prediction, target):\n",
    "        return prediction - target\n",
    "class CELoss(Loss):\n",
    "    def forward(self, prediction, target):\n",
    "        if len(prediction.shape) == 1:\n",
    "            return -target.dot(np.log(prediction))\n",
    "        else:\n",
    "            return np.array([-ystar.dot(np.log(y)) for ystar, y in zip(target, prediction)])\n",
    "    def backward(self, prediction, target):\n",
    "        assert type(target) == csr_array\n",
    "        # Make prediction matrix-shaped if it was a vector\n",
    "        if len(prediction.shape) == 1: prediction = prediction[None, :]\n",
    "        delta_data = np.zeros(target.data.shape)\n",
    "        for i_data, (i, j, value) in enumerate(zip(target.indptr, target.indices, target.data)):\n",
    "            delta_data[i_data] = - value / prediction[i,j]\n",
    "        return csr_array((delta_data, target.indices, target.indptr), shape=prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632b4ad",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "In the lecture, you were introduced to the Scaled Dot-Product Attention, which is defined as\n",
    "\n",
    "$$SDPA(Q,K,V) = Softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\cdot V$$\n",
    "\n",
    "where $d_k$ is the dimension of queries $Q$, keys $K$, and values $V$, sometimes called the latent dimension of the language model.\n",
    "Note the shapes of the terms:\n",
    "- $Q, K$, and $V$ are $n \\times d_k$ matrices representing $n$ vectors (e.g. word embeddings) of dimension $d_k$\n",
    "- $QK^T$ is an $n \\times n$ matrix that denotes the importance of some key $K_j$ for some query $Q_i$.\n",
    "- $SDPA(Q,K,V)$ is an $n \\times m$ matrix that aggregates the values $V_j$ that correspond to each key $K_j$ according to their importance for the query $Q_i$.\n",
    "\n",
    "For simplicity, we can decompose $SDPA$ into the steps\n",
    "- Transposing $K$\n",
    "- Dot Product of $Q$ and $K^T$\n",
    "- Scalar Multiplication of $QK^T$ with $d_k^{-\\frac{1}{2}}$\n",
    "- Dense Softmax of $\\frac{QK^T}{\\sqrt{d_k}}$\n",
    "- Dot Product of $Softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$ and $V$\n",
    "\n",
    "To represent these steps in our framework, we need the additional modules `Transpose`, `DotProduct` and `ScalarMultiplication`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(NetworkModule):\n",
    "    def _forward(self, X):\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        pass # Your solution here\n",
    "class ScalarProduct(NetworkModule):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "    def _forward(self, X):\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        pass # Your solution here\n",
    "class DotProduct(NetworkModule):\n",
    "    def _forward(self, X):\n",
    "        # Here X should be a tuple, list or similar (A, B)\n",
    "        # of vectors or matrices to take the numpy.dot product off\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        # This must return a tuple (delta_A, delta_B)\n",
    "        # which contains the appropriate delta for each\n",
    "        # of the inputs.\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505cf64",
   "metadata": {},
   "source": [
    "Using these new modules, you can define the $SDPA$ as an \"activation function\", that receives triplets $(Q,K,V)$ during `forward` and sends triplets $(\\delta_Q,\\delta_K,\\delta_V)$ during `backward`. Receive the latent dimension $d_k$ in the constructor to initialize the `ScalarProduct` module with.\n",
    "\n",
    "You can use the `numpy_zip` function to join multiple arrays of the same shape into a joined array. For example `numpy_zip(A,B,C)` for arrays `A`, `B`, `C` of shape `(5,10)` will be returned as an array of shape `(3,5,10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_zip(*arrays):\n",
    "    return np.concatenate([array[None] for array in arrays], axis=0)\n",
    "class SDPA(NetworkModule):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super().__init__()\n",
    "        self.latent_dimension = latent_dimension\n",
    "        # self.transpose = ...\n",
    "        # self.dot_product_qk = ...\n",
    "        # self.normalization = ...\n",
    "        # self.softmax = ...\n",
    "        # self.dot_product_v = ...\n",
    "        pass # Your solution here\n",
    "    def _forward(self, X):\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa09cc9",
   "metadata": {},
   "source": [
    "Use the code below to debug your solution. The code initializes random query, key, and value matrices and a random `target` matrix.\n",
    "Then back propagation is used to compute the gradients for $Q$, $K$, and $V$ such that $SDPA(Q,K,V)$ approximates the desired `target` matrix.\n",
    "For a working implementation, the loss value should be approximately 0 after 2000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2055f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, K, V = np.random.sample((3, 5, 10))\n",
    "attention = SDPA(Q.shape[1])\n",
    "target = np.random.sample((5,10))\n",
    "loss = L2Loss()\n",
    "learning_rate = 1e-1\n",
    "loss_values = []\n",
    "for _ in tqdm(range(2000)):\n",
    "\tpredicted = attention.forward(numpy_zip(Q, K, V))\n",
    "\tloss_value = loss.forward(predicted, target)\n",
    "\tloss_values.append(np.sum(loss_value))\n",
    "\tgradient = attention.backward(loss.backward(predicted, target))\n",
    "\tQ -= gradient[0] * learning_rate\n",
    "\tK -= gradient[1] * learning_rate\n",
    "\tV -= gradient[2] * learning_rate\n",
    "go.Figure(go.Scatter(y=loss_values)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1637cb70",
   "metadata": {},
   "source": [
    "Using the helper classes `InputFork`, which repeats an input and returns it as a list, and `SkipAddNorm`, which creates a skip connection around a given module and adds and normalizes its outputs with the inputs, you can fill out the classes `PremultipliedSDPA` and `MultiHeadAttention`.\n",
    "\n",
    "`PremultipliedSDPA` should contain the three linear modules to transform the inputs $Q$, $K$, and $V$ and an `SDPA`.\n",
    "During `forward`, split the input `X` into the three inputs, apply the linear transforms and forward the results through the `SDPA`.\n",
    "\n",
    "`MultiHeadAttention` should fork the input into `n_heads` copies, forward them through each of the `PremultipliedSDPA` heads, concatenate their results and run it through a `Linear` module to reduce the dimension back down to the `latent_dimension`.\n",
    "Make use of the parent class `ModuleChain` to reduce the required code as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFork(NetworkModule):\n",
    "    def __init__(self, n_forks):\n",
    "        self.n_forks = n_forks\n",
    "    def _forward(self, X):\n",
    "        return [X for _ in range(self.n_forks)]\n",
    "    def backward(self, delta):\n",
    "        return np.sum(delta,axis=0)\n",
    "class SkipAddNorm(NetworkModule):\n",
    "    def __init__(self, skipped_module):\n",
    "        super().__init__()\n",
    "        self.skipped_module = skipped_module\n",
    "        self.stds = None\n",
    "        self.n = None\n",
    "        self.centered = None\n",
    "    def _forward(self, X):\n",
    "        X2 = self.skipped_module.forward(X)\n",
    "        added = X + X2\n",
    "        means = np.mean(added, axis=-1, keepdims=True)\n",
    "        self.centered = added - means\n",
    "        self.stds = np.sqrt(np.mean(np.square(self.centered), axis=-1, keepdims=True))\n",
    "        # Adding a small epsilon to avoid division by zero\n",
    "        self.stds += 1e-10\n",
    "        self.n = self.centered.shape[-1]\n",
    "        standardized_centered = self.centered / self.stds\n",
    "        return standardized_centered\n",
    "    def backward(self, delta):\n",
    "        assert self.n == delta.shape[-1]\n",
    "        pre_standardized_delta = np.zeros(delta.shape)\n",
    "        for i,(x,d,std) in enumerate(zip(self.last_input, delta, self.stds)):\n",
    "            jacobian = (\n",
    "                (self.n * np.eye(self.n) - 1) / (self.n * std)\n",
    "                - np.outer(x,x) / (self.n * std**3)\n",
    "            )\n",
    "            pre_standardized_delta[i] = jacobian.dot(d)\n",
    "        skipped_module_delta = self.skipped_module.backward(pre_standardized_delta)\n",
    "        return pre_standardized_delta + skipped_module_delta\n",
    "    def step_gradient(self, learning_rate):\n",
    "        self.skipped_module.step_gradient(learning_rate)\n",
    "class PremultipliedSDPA(NetworkModule):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super().__init__()\n",
    "        self.latent_dimension = latent_dimension\n",
    "        # self.linears = ...\n",
    "        # self.sdpa = ...\n",
    "        pass # Your solution here\n",
    "    def _forward(self, X):\n",
    "        Q,K,V = X\n",
    "        pass # Your solution here\n",
    "    def backward(self, delta):\n",
    "        # Should return a triplet of deltas for Q, K, and V respectively.\n",
    "        pass # Your solution here\n",
    "    def step_gradient(self, learning_rate):\n",
    "        pass # Your solution here\n",
    "class MultiHeadAttention(ModuleChain):\n",
    "    def __init__(self, latent_dimension, n_heads):\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.n_heads = n_heads\n",
    "        # Call the super constructor\n",
    "        # super().__init__([...])\n",
    "        pass # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e617f3",
   "metadata": {},
   "source": [
    "The cell below contains some code to test, whether or not backpropagation through your `MultiHeadAttention` class works.\n",
    "A single set of inputs is matched against a single output and the weights inside the `MultiHeadAttention` are changed to achieve the desired output.\n",
    "The loss should approach 0 within the 10k gradient steps, although the approximation quality can vary due to the random initialization.\n",
    "If the loss is far from 0 for multiple executions, you probably still have a bug in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec40c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, K, V = np.random.normal(0,1,(3,20,5))\n",
    "attention = MultiHeadAttention(Q.shape[1], 5)\n",
    "target = np.random.sample(Q.shape)\n",
    "loss = L2Loss()\n",
    "learning_rate = 1e-1\n",
    "loss_values = []\n",
    "for _ in tqdm(range(10_000)):\n",
    "\tpredicted = attention.forward(numpy_zip(Q, K, V))\n",
    "\tloss_value = loss.forward(predicted, target)\n",
    "\tloss_values.append(np.sum(loss_value))\n",
    "\tgradient = attention.backward(loss.backward(predicted, target))\n",
    "\tattention.step_gradient(learning_rate)\n",
    "go.Figure(go.Scatter(y=loss_values)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199b740",
   "metadata": {},
   "source": [
    "We will again make use of the `OneHotDict` to go from tokens to vectors and back. The class is the same as in the previous notebook but adapted to handling multiple words at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1231cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotDict():\n",
    "    def __init__(self, vocabulary):\n",
    "        # Ensure appropriate types for ordered access\n",
    "        self.vocabulary = np.array(list(vocabulary))\n",
    "        self.lookup = {k:v for v,k in enumerate(vocabulary)}\n",
    "    def word_to_one_hot(self, word):\n",
    "        assert word in self.lookup.keys()\n",
    "        return csr_array(([1.], ([0],[self.lookup[word]])), shape=(1, len(self.vocabulary)))\n",
    "    def words_to_one_hot(self, words):\n",
    "        assert all(word in self.lookup.keys() for word in words)\n",
    "        return csr_array(\n",
    "            (np.ones(len(words)), (np.arange(len(words)),[self.lookup[word] for word in words])),\n",
    "            shape=(len(words), len(self.vocabulary))\n",
    "        )\n",
    "    def one_hot_to_word(self, one_hot):\n",
    "        return self.one_hot_to_words(one_hot)[0]\n",
    "    def one_hot_to_words(self, one_hot):\n",
    "        assert one_hot.shape[-1] == len(self.vocabulary)\n",
    "        return self.vocabulary[one_hot.argmax(axis=-1).flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f84ed0",
   "metadata": {},
   "source": [
    "Once again, we load the Hamlet text, since it contains only quite few different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77792af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbbootstrap\n",
    "file_path = await nbbootstrap.ensure_resource(\"https://dm.cs.tu-dortmund.de/nats/data/hamlet.txt\")\n",
    "with open(file_path, \"rt\") as file:\n",
    "    full = file.read()\n",
    "import re\n",
    "sentence_regex = re.compile(r\"[.?!]|\\n\\n+\")\n",
    "words_regex = re.compile(r\"[\\w']+\", re.U)\n",
    "special_chars = \".:,;?!-_\\\"'()„“”‚‘’…\"\n",
    "padding_token = \"_\" # This character can not occur in any of the words.\n",
    "k_tokens = 7 # How many tokens to base the next word on.\n",
    "tokenized_sentences = [] # Store your output in this list.\n",
    "# First split Hamlet into sentences, then tokenize each sentence.\n",
    "for sentence in sentence_regex.split(full):\n",
    "    for c in special_chars: sentence = sentence.replace(c, \"\")\n",
    "    sentence = sentence.strip()\n",
    "    if sentence == \"\": continue\n",
    "    tokenized = []\n",
    "    for w in words_regex.findall(sentence):\n",
    "        tokenized.append(w.lower())\n",
    "    if len(tokenized) <= 2: continue\n",
    "    tokenized_sentences.append([\n",
    "        *[padding_token for _ in range(k_tokens)],\n",
    "        *tokenized,\n",
    "        padding_token,\n",
    "    ])\n",
    "ohd = OneHotDict(np.unique([token for sentence in tokenized_sentences for token in sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4a1f9",
   "metadata": {},
   "source": [
    "As positional encoding, we will make use of the RoPE rotary embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(NetworkModule):\n",
    "    def __init__(self, latent_dimension, max_pos=100):\n",
    "        self.thetas = 10_000**(-2*np.arange(latent_dimension//2)/(latent_dimension//2))\n",
    "        self.sines = np.sin(np.outer(np.arange(max_pos), self.thetas))\n",
    "        self.cosines = np.cos(np.outer(np.arange(max_pos), self.thetas))\n",
    "        self.rotation_matrices = np.array([\n",
    "            [\n",
    "                [\n",
    "                    [self.cosines[m,i], -self.sines[m,i]],\n",
    "                    [self.sines[m,i], self.cosines[m,i]],\n",
    "                ]\n",
    "                for i in range(latent_dimension//2)\n",
    "            ]\n",
    "            for m in range(max_pos)\n",
    "        ])\n",
    "        self.position = 0\n",
    "    def set_position(self, position):\n",
    "        self.position = position\n",
    "    def _forward(self, X):\n",
    "        result = np.zeros(X.shape)\n",
    "        for pos in range(X.shape[0]):\n",
    "            for i in range(X.shape[1]//2):\n",
    "                result[pos, 2*i:2*(i+1)] = X[pos, 2*i:2*(i+1)].dot(\n",
    "                    self.rotation_matrices[self.position+pos,i]\n",
    "                )\n",
    "        return result\n",
    "    def backward(self, delta):\n",
    "        result = np.zeros(delta.shape)\n",
    "        for pos in range(delta.shape[0]):\n",
    "            for i in range(delta.shape[1]//2):\n",
    "                result[pos, 2*i:2*(i+1)] = delta[pos, 2*i:2*(i+1)].dot(\n",
    "                    self.rotation_matrices[self.position+pos,i].T\n",
    "                )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf1573",
   "metadata": {},
   "source": [
    "To faster obtain a reasonable result, we will start with the pretrained word vectors obtained from last weeks assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbbootstrap\n",
    "file_path = await nbbootstrap.ensure_resource(\"https://dm.cs.tu-dortmund.de/nats/data/model_2_words_100_dim.json.gz\")\n",
    "import gzip, json\n",
    "with gzip.open(file_path) as f:\n",
    "\told_model = json.load(f)\n",
    "init_vecs = np.array(old_model[\"model\"][\"children\"][0][\"children\"][0][\"weights\"])\n",
    "# Overwriting the vocabulary to ensure a proper match between indices and vectors.\n",
    "# As long as you did not change anything about the text parsing above, this should work.\n",
    "ohd = OneHotDict(old_model[\"vocabulary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c5c20",
   "metadata": {},
   "source": [
    "We can finally create the multi headed attention model.\n",
    "The latent dimension is prescribed by the pre-trained word vectors, whilst the number of `MultiHeadedAttention`-blocks and the number of heads per block can be parameterized.\n",
    "We will again enforce normalized word vectors to avoid diverging model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f553d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_dim = init_vecs.shape[1]\n",
    "n_heads = 5\n",
    "n_blocks = 2\n",
    "make_decoder_block = lambda n_heads: ModuleChain([\n",
    "    SkipAddNorm(ModuleChain([\n",
    "        InputFork(3),\n",
    "        MultiHeadAttention(word_vec_dim, n_heads)\n",
    "    ])),\n",
    "    SkipAddNorm(ModuleChain([\n",
    "        Linear(word_vec_dim, word_vec_dim),\n",
    "        ReLU(),\n",
    "        Linear(word_vec_dim, word_vec_dim),\n",
    "    ])),\n",
    "])\n",
    "rotary_embedding = RotaryEmbedding(word_vec_dim, max_pos = 2*max(len(s) for s in tokenized_sentences))\n",
    "decoder_only_model = ModuleChain([\n",
    "    Linear(len(ohd.vocabulary), word_vec_dim),\n",
    "    rotary_embedding,\n",
    "    *[make_decoder_block(n_heads) for _ in range(n_blocks)],\n",
    "    Linear(word_vec_dim, len(ohd.vocabulary)),\n",
    "    SparseSoftmax(),\n",
    "])\n",
    "decoder_only_model.modules[0].weights = init_vecs.copy()\n",
    "# Normalize embeddings\n",
    "decoder_only_model.modules[0].weights /= np.linalg.norm(decoder_only_model.modules[0].weights,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b777989",
   "metadata": {},
   "source": [
    "Below is the training pipeline. We again sample random positions in random sentences and predict the next word by having inputs and outputs shifted by one token.\n",
    "You can modify the code at your convenience, run it and see how the model slowly approaches the expected outputs, first in the padding tokens and later in (similar) tokens.\n",
    "\n",
    "To obtain good results, you will probably have to run this for way more than $10\\,000$ iterations.\n",
    "This is likely not viable in the browser, but you can run it at least for long enough to observe how the model transitions from entirely random tokens to somewhat resembling the expected output.\n",
    "On repeated executions of this cell, you may want to comment out the code that resets the word vectors to the pretrained ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Label\n",
    "from IPython.display import display\n",
    "np.seterr(divide='raise', invalid='raise')\n",
    "loss = CELoss()\n",
    "learning_rate = 1e-1\n",
    "floating_loss = None\n",
    "float_val = 0.9999\n",
    "expected_label = Label(value=\"Expected:\",continuous_update=True)\n",
    "predicted_label = Label(value=\"Predicted:\",continuous_update=True)\n",
    "probs_label = Label(value=\"True probabilities:\",continuous_update=True)\n",
    "display(expected_label)\n",
    "display(predicted_label)\n",
    "display(probs_label)\n",
    "with tqdm(range(10_000)) as bar:\n",
    "    for it in bar:\n",
    "        sentence = tokenized_sentences[np.random.randint(len(tokenized_sentences))]\n",
    "        offset = np.random.randint(min(len(sentence)-k_tokens-2, k_tokens-1),len(sentence)-k_tokens-1)\n",
    "        # Set rotary encoding position\n",
    "        decoder_only_model.modules[1].set_position(offset)\n",
    "        inputs = ohd.words_to_one_hot(sentence[offset:][:k_tokens])\n",
    "        outputs = ohd.words_to_one_hot(sentence[offset:][1:k_tokens+1])\n",
    "        predicted = decoder_only_model.forward(inputs)\n",
    "        decoder_only_model.backward(loss.backward(predicted, outputs))\n",
    "        decoder_only_model.step_gradient(learning_rate)\n",
    "        # Reset the embedding vectors to the pretrained vectors during burn-in.\n",
    "        if it < 1_000:\n",
    "            decoder_only_model.modules[0].weights = init_vecs.copy()\n",
    "        # Normalize embeddings\n",
    "        decoder_only_model.modules[0].weights /= np.linalg.norm(decoder_only_model.modules[0].weights,axis=1,keepdims=True)\n",
    "        loss_value = loss.forward(predicted, outputs)\n",
    "        floating_loss = np.mean(loss_value) if floating_loss is None else floating_loss*float_val + np.mean(loss_value)*(1-float_val)\n",
    "        bar.desc = f\"Floating loss: {floating_loss:>8.4f}\"\n",
    "        if it%20 == 0:\n",
    "            expected_label.value = f\"Expected: '{' '.join(sentence[offset:][1:k_tokens+1])}'\"\n",
    "            predicted_label.value = f\"Predicted: '{' '.join(ohd.one_hot_to_words(predicted))}'\"\n",
    "            probs_label.value = f\"True probabilities: {', '.join(list(map(lambda v: f'{v:.5f}', (outputs * predicted).max(axis=1).toarray().flatten())))}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94bd9b",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Add code to the cell below to visualize the attention each head puts on each input word to predict the output words.\n",
    "The token-to-token attention is the matrix produced by the `DenseSoftmax` inside `SDPA`s.\n",
    "You will thus have to forward the random input through the individual parts of the model manually to obtain these matrices (or use the input caching of the modules).\n",
    "\n",
    "Run the cell for each head (at least of the first block) multiple times and try to figure out, what each head is paying attention to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd09267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to choose what block and what head inside that block\n",
    "# to visualize token-to-token attention for.\n",
    "inspected_block = 0\n",
    "# Selecting a random inputs-outputs-pair.\n",
    "sentence = tokenized_sentences[np.random.randint(len(tokenized_sentences))]\n",
    "offset = np.random.randint(min(len(sentence)-k_tokens-2, k_tokens-1), len(sentence)-k_tokens-1)\n",
    "decoder_only_model.modules[1].set_position(offset)\n",
    "inputs = ohd.words_to_one_hot(sentence[offset:][:k_tokens])\n",
    "outputs = ohd.words_to_one_hot(sentence[offset:][1:k_tokens+1])\n",
    "# Compute the attention of the specified head\n",
    "# importance = ... <- this should be the matrix of token-to-token attention\n",
    "#                     where each row belongs to an output word and each\n",
    "#                     column belongs to an input word.\n",
    "for inspected_head in range(len(decoder_only_model.modules[2].modules[0].skipped_module.modules[1].modules[1].modules)):\n",
    "    pass # Your solution here\n",
    "    # Preparing the input and output words for visualization.\n",
    "    # The lists need to be duplicate free for plotly visualization,\n",
    "    # hence, we add some whitespaces to make all tokens \"different\"\n",
    "    words_in = sentence[offset:][:k_tokens]\n",
    "    cnts = {k:0 for k in set(words_in)}\n",
    "    for i in range(len(words_in)):\n",
    "        cnts[words_in[i]] += 1\n",
    "        words_in[i] = f\"{words_in[i]}{' '*cnts[words_in[i]]}\"\n",
    "    words_out = sentence[offset:][1:k_tokens+1]\n",
    "    cnts = {k:0 for k in set(words_out)}\n",
    "    cnts[padding_token] = 1\n",
    "    for i in range(len(words_out)):\n",
    "        cnts[words_out[i]] += 1\n",
    "        words_out[i] = f\"{words_out[i]}{' '*cnts[words_out[i]]}\"\n",
    "    # Visualize the token-to-token attention in that head.\n",
    "    go.Figure(\n",
    "        go.Heatmap(x=words_in,y=words_out,z=importance),\n",
    "        layout_yaxis_scaleanchor=\"x\",\n",
    "        layout_yaxis_title=\"Expected tokens\",\n",
    "        layout_xaxis_title=\"Input tokens\",\n",
    "        layout_title=f\"Attention of head {inspected_head} in block {inspected_block}\",\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca0ff5",
   "metadata": {},
   "source": [
    "You can also use the model to produce sentences similar to the model of last week.\n",
    "Start with `k_tokens` times the `padding_token` and feed the attention model with the last `k_tokens` produced tokens.\n",
    "Sample from the probability distribution *of the last output vector* to produce the next token.\n",
    "Add the token to the `generated_words` and repeat.\n",
    "\n",
    "Attention models are somewhat famous for not having a fixed length, yet being sequence-to-sequence models. What happens if you change the number of input tokens?\n",
    "\n",
    "**Take care:** Remember to set the position of the `rotary_embedding`! The position must be the 0-based index of the first input token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17877ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "generated_words = [padding_token for _ in range(k_tokens)]\n",
    "# generated_words += \"hamlet and his\".split(\" \")\n",
    "for word in generated_words:\n",
    "\tif word != padding_token: print(word, end=\" \")\n",
    "while (all(v==padding_token for v in generated_words) or generated_words[-1] != padding_token) and len(generated_words) < 100:\n",
    "\t# Process:\n",
    "\t# - set the rotary embedding start position to the number of tokens *before* the first input token\n",
    "\t# - feed forward the last k words as one-hot vectors\n",
    "\t# - sample word according to predicted probabilities\n",
    "\t# - append word to sentence print the word\n",
    "\tpass # Your solution here\n",
    "\tgenerated_words.append(next_word)\n",
    "\tif next_word != padding_token: print(next_word, end=\" \")\n",
    "print()\n",
    "print(\"Generated\",len(generated_words),\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a633fc",
   "metadata": {},
   "source": [
    "You can of course also use evaluation on the word vectors obtained after training.\n",
    "We initialized the vectors to pre-trained vectors, but the backpropagation is allowed to modify the word embeddings.\n",
    "How meaningful do you think these are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some stuff\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2069d32",
   "metadata": {},
   "source": [
    "## Model IO\n",
    "\n",
    "To save your model or load a stored state, you can use the cells below.\n",
    "Pickling is a very easy-to-use way of saving models, but somewhat vulnerable to changes in the class code.\n",
    "Writing a more robust IO code is generally advised (e.g. the JSON to gzip pipeline used for the pretrained vectors), but that takes significantly more effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a model\n",
    "import pickle\n",
    "with open(\"path_to_model.pkl\", \"wb\") as f: pickle.dump(decoder_only_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d10e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a model\n",
    "import pickle\n",
    "with open(\"path_to_model.pkl\", \"rb\") as f: decoder_only_model = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
