{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scikit-learn pandas\n",
    "%pip install --no-cache-dir --force-reinstall https://dm.cs.tu-dortmund.de/nats/nats25_08_01_bert_embeddings-0.1-py3-none-any.whl\n",
    "import nats25_08_01_bert_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6455980",
   "metadata": {},
   "source": [
    "# Deep Neural Embeddings\n",
    "\n",
    "In this assignment, we will work with deep neural embeddings (but not train such an embedding, which is much too ressource intensive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc680f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import json, gzip, urllib\n",
    "# Load dataset\n",
    "file_path, _ = urllib.request.urlretrieve(\"https://dm.cs.tu-dortmund.de/nats/data/minecraft-articles.json.gz\")\n",
    "with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as file:\n",
    "\traw = json.load(file)\n",
    "titles, texts, classes, mclasses = [x[\"title\"] for x in raw], [x[\"text\"] for x in raw], [x[\"heuristic\"] for x in raw], [x[\"transitive\"] for x in raw]\n",
    "# Free memory\n",
    "del raw\n",
    "# Load BERT mean vectors\n",
    "file_path, _ = urllib.request.urlretrieve(\"https://dm.cs.tu-dortmund.de/nats/data/minecraft-bge-m3.npy\")\n",
    "vectors = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "vectors = sklearn.preprocessing.normalize(vectors, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec371b7",
   "metadata": {},
   "source": [
    "## Cluster deep vectors with k-means\n",
    "\n",
    "Find the \"best\" result when clustering with k-means for k=2..10 by ARI.\n",
    "\n",
    "For reproducibility, use the fixed random seed 0, 1 restarts, and no tolerance.\n",
    "\n",
    "Log for yourself the time needed to cluster.\n",
    "\n",
    "Note that on real data, we cannot use ARI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "best = (0, -1, None, None) # ARI, k, assignment, centers\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090566c",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_bert_embeddings.hidden_tests_5_0(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82d390",
   "metadata": {},
   "source": [
    "# Explore the clustering\n",
    "\n",
    "Explore the clustering: print each clusters size and the 5 most central documents to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(ari, k, assignment, centers):\n",
    "    print(\"ARI:\", ari)\n",
    "    pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a01280",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain(*best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc35a15",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_bert_embeddings.hidden_tests_9_0(explain, best, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5570c",
   "metadata": {},
   "source": [
    "## Improve the cluster explanation with TF-IDF\n",
    "\n",
    "Interestingly, TF-IDF is still useful here - what are the important words, now that we only work with 768-dimensional mean vectors? These averaged vectors are not very similar to word vectors anymore (all close together, and close to stop words).\n",
    "\n",
    "First, get back our old tf-idf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tfidf vectors!\n",
    "tfidf = None # sparse tf-idf matrix\n",
    "vocabulary = None # vocabulary\n",
    "idf = None # IDF values\n",
    "pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ac8fb",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_bert_embeddings.hidden_tests_12_0(vectors, vocabulary, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b26e0e6",
   "metadata": {},
   "source": [
    "Now write an explain2 function that also prints the most important words for each cluster.\n",
    "\n",
    "Also use the multi-classifiction information in mclasses to explain the cluster contents in terms of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb123d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain2(ari, k, assignment, centers, tfidf, idf, vocabulary):\n",
    "    print(\"ARI:\", ari)\n",
    "    pass # Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain2(*best, tfidf, idf, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321aeda",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "nats25_08_01_bert_embeddings.hidden_tests_16_0(vocabulary, mclasses, tfidf, best, idf, explain2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
